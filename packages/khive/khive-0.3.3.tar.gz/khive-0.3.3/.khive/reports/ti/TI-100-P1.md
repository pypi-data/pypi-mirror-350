---
title: Test Implementation Plan for API Client Architecture Refactor - Phase 1
by: khive-implementer
created: 2025-05-18
updated: 2025-05-18
version: 1.0
doc_type: TI
output_subdir: ti
description: "Test implementation plan for Phase 1 of the API Client Architecture Refactor (Issue #100)"
date: 2025-05-18
---

# Test Implementation Plan: API Client Architecture Refactor - Phase 1

## 1. Overview

### 1.1 Component Under Test

This test implementation plan covers the components being refactored in Phase 1
of the API Client Architecture Refactor (Issue #100):

1. `APICalling` - New event class for API interactions
2. `Queue` and `QueueConfig` - Refactored queue implementation
3. `Executor` and `RateLimitedExecutor` - Refactored executor implementations
4. `Endpoint` - Updated for direct HTTP client management
5. `AsyncAPIClient` - Deprecated/removed or role significantly reduced

### 1.2 Test Approach

We will use a combination of:

- **Unit Tests**: For individual component functionality
- **Integration Tests**: For component interactions
- **Resource Management Tests**: To ensure proper cleanup of async resources
- **Concurrency Tests**: To verify proper handling of concurrent operations

All tests will follow the TDD approach, with tests written before
implementation.

### 1.3 Key Testing Goals

- Verify that the new `APICalling` event class correctly represents and executes
  API calls
- Ensure the refactored `Queue` and `Executor` components maintain or improve
  existing functionality
- Verify that `Endpoint` correctly manages HTTP client lifecycle
- Ensure proper resource cleanup in all async components
- Verify rate limiting functionality in `RateLimitedExecutor`
- Ensure backward compatibility where required

## 2. Test Environment

### 2.1 Test Framework

```
pytest
pytest-asyncio
pytest-mock
pytest-cov
```

### 2.2 Mock Framework

```
unittest.mock
pytest-mock
```

### 2.3 Test Database

No database is required for these tests. All external dependencies will be
mocked.

## 3. Unit Tests

### 3.1 Test Suite: APICalling

#### 3.1.1 Test Case: Initialization

**Purpose:** Verify that APICalling is correctly initialized with the provided
parameters.

**Setup:**

```python
@pytest.fixture
def mock_endpoint():
    return Mock(spec=Endpoint)
```

**Test Implementation:**

```python
def test_api_calling_initialization(mock_endpoint):
    # Arrange
    request = {"param": "value"}

    # Act
    api_call = APICalling(
        endpoint=mock_endpoint,
        request=request,
        cache_control=True,
        requires_tokens=True
    )

    # Assert
    assert api_call.requires_tokens is True
    assert api_call.event_type == "api_calling"
    assert api_call.event_invoke_function == mock_endpoint.call
    assert api_call.event_invoke_kwargs == {
        "request": request,
        "cache_control": True
    }
```

#### 3.1.2 Test Case: Required Tokens Property

**Purpose:** Verify that the required_tokens property works correctly.

**Test Implementation:**

```python
def test_api_calling_required_tokens(mock_endpoint):
    # Arrange
    api_call = APICalling(
        endpoint=mock_endpoint,
        request={},
        requires_tokens=True
    )

    # Act
    api_call.required_tokens = 10

    # Assert
    assert api_call.required_tokens == 10
```

#### 3.1.3 Test Case: Invoke Method

**Purpose:** Verify that the invoke method correctly calls the endpoint.

**Test Implementation:**

```python
async def test_api_calling_invoke(mock_endpoint):
    # Arrange
    mock_endpoint.call = AsyncMock(return_value={"result": "success"})
    request = {"param": "value"}
    api_call = APICalling(
        endpoint=mock_endpoint,
        request=request
    )

    # Act
    await api_call.invoke()

    # Assert
    mock_endpoint.call.assert_called_once_with(request=request, cache_control=False)
    assert api_call.execution.status == ExecutionStatus.COMPLETED
    assert api_call.execution.result == {"result": "success"}
```

### 3.2 Test Suite: Queue

#### 3.2.1 Test Case: QueueConfig Validation

**Purpose:** Verify that QueueConfig validates parameters correctly.

**Test Implementation:**

```python
def test_queue_config_validation():
    # Valid configuration
    config = QueueConfig(queue_capacity=100, capacity_refresh_time=1.0)
    assert config.queue_capacity == 100
    assert config.capacity_refresh_time == 1.0

    # Invalid queue_capacity
    with pytest.raises(ValueError):
        QueueConfig(queue_capacity=0, capacity_refresh_time=1.0)

    # Invalid capacity_refresh_time
    with pytest.raises(ValueError):
        QueueConfig(queue_capacity=100, capacity_refresh_time=0)
```

#### 3.2.2 Test Case: Queue Initialization

**Purpose:** Verify that Queue is correctly initialized with the provided
parameters.

**Test Implementation:**

```python
def test_queue_initialization():
    # Arrange & Act
    queue = Queue(
        queue_capacity=100,
        capacity_refresh_time=1.0,
        concurrency_limit=5
    )

    # Assert
    assert queue.queue_capacity == 100
    assert queue.capacity_refresh_time == 1.0
    assert queue._concurrency_sem is not None
```

#### 3.2.3 Test Case: Queue Enqueue and Dequeue

**Purpose:** Verify that Queue correctly enqueues and dequeues events.

**Setup:**

```python
@pytest.fixture
def mock_event():
    return Mock(spec=Event)
```

**Test Implementation:**

```python
async def test_queue_enqueue_dequeue(mock_event):
    # Arrange
    queue = Queue(
        queue_capacity=100,
        capacity_refresh_time=1.0
    )
    await queue.start()

    # Act
    await queue.enqueue(mock_event)
    dequeued_event = await queue.dequeue()

    # Assert
    assert dequeued_event == mock_event
    assert queue.unfinished_tasks == 1

    # Cleanup
    queue.task_done()
    await queue.stop()
```

#### 3.2.4 Test Case: Queue Concurrency Control

**Purpose:** Verify that Queue correctly applies concurrency limits.

**Test Implementation:**

```python
async def test_queue_concurrency_control():
    # Arrange
    concurrency_limit = 2
    queue = Queue(
        queue_capacity=100,
        capacity_refresh_time=1.0,
        concurrency_limit=concurrency_limit
    )
    await queue.start()

    # Act & Assert
    # Create tasks that will block on dequeue
    events = [Mock(spec=Event) for _ in range(concurrency_limit + 1)]
    for event in events:
        await queue.enqueue(event)

    # First concurrency_limit dequeues should succeed immediately
    dequeued_events = []
    for _ in range(concurrency_limit):
        dequeued_events.append(await queue.dequeue())

    # Next dequeue should block until we call task_done
    dequeue_task = asyncio.create_task(queue.dequeue())

    # Give the task a chance to run, but it should still be pending
    await asyncio.sleep(0.1)
    assert not dequeue_task.done()

    # After calling task_done, the dequeue should complete
    queue.task_done()
    await asyncio.wait_for(dequeue_task, timeout=1.0)

    # Cleanup
    for _ in range(concurrency_limit):
        queue.task_done()
    await queue.stop()
```

### 3.3 Test Suite: Executor

#### 3.3.1 Test Case: Executor Initialization

**Purpose:** Verify that Executor is correctly initialized with the provided
parameters.

**Test Implementation:**

```python
def test_executor_initialization():
    # Arrange & Act
    executor = Executor(
        event_type=Event,
        queue_config=QueueConfig(
            queue_capacity=100,
            capacity_refresh_time=1.0,
            concurrency_limit=5
        )
    )

    # Assert
    assert executor.event_type == Event
    assert executor.queue_config.queue_capacity == 100
    assert executor.queue_config.capacity_refresh_time == 1.0
    assert executor.queue_config.concurrency_limit == 5
    assert executor.task_queue is None
    assert len(executor.pending) == 0
    assert len(executor.events) == 0
```

#### 3.3.2 Test Case: Executor Append and Pop

**Purpose:** Verify that Executor correctly appends and pops events.

**Setup:**

```python
@pytest.fixture
def mock_event():
    event = Mock(spec=Event)
    event.id = uuid.uuid4()
    return event
```

**Test Implementation:**

```python
def test_executor_append_pop(mock_event):
    # Arrange
    executor = Executor(
        event_type=Event,
        queue_config=QueueConfig(
            queue_capacity=100,
            capacity_refresh_time=1.0
        )
    )

    # Act
    executor.append(mock_event)

    # Assert
    assert mock_event.id in executor.events
    assert mock_event.id in executor.pending

    # Act
    popped_event = executor.pop(mock_event.id)

    # Assert
    assert popped_event == mock_event
    assert mock_event.id not in executor.events
```

#### 3.3.3 Test Case: Executor Process Event

**Purpose:** Verify that Executor correctly processes events.

**Test Implementation:**

```python
async def test_executor_process_event(mock_event):
    # Arrange
    executor = Executor(
        event_type=Event,
        queue_config=QueueConfig(
            queue_capacity=100,
            capacity_refresh_time=1.0
        )
    )
    mock_event.invoke = AsyncMock()
    mock_event.execution = Mock()

    # Act
    await executor.process_event(mock_event)

    # Assert
    mock_event.invoke.assert_called_once()
```

### 3.4 Test Suite: RateLimitedExecutor

#### 3.4.1 Test Case: RateLimitedExecutor Initialization

**Purpose:** Verify that RateLimitedExecutor is correctly initialized with the
provided parameters.

**Test Implementation:**

```python
def test_rate_limited_executor_initialization():
    # Arrange & Act
    executor = RateLimitedExecutor(
        queue_capacity=100,
        capacity_refresh_time=1.0,
        interval=2.0,
        limit_requests=10,
        limit_tokens=1000,
        concurrency_limit=5
    )

    # Assert
    assert executor.limit_requests == 10
    assert executor.limit_tokens == 1000
    assert executor.interval == 2.0
    assert executor.available_request == 10
    assert executor.available_token == 1000
```

#### 3.4.2 Test Case: RateLimitedExecutor Request Permission - Request Limits

**Purpose:** Verify that RateLimitedExecutor correctly applies request limits.

**Setup:**

```python
@pytest.fixture
def mock_api_calling():
    event = Mock(spec=APICalling)
    event.id = uuid.uuid4()
    event.required_tokens = None
    return event
```

**Test Implementation:**

```python
async def test_rate_limited_executor_request_permission_request_limits(mock_api_calling):
    # Arrange
    executor = RateLimitedExecutor(
        queue_capacity=100,
        capacity_refresh_time=1.0,
        limit_requests=2,
        limit_tokens=None
    )

    # Act & Assert
    # First two requests should be granted
    assert await executor.request_permission(mock_api_calling) is True
    assert await executor.request_permission(mock_api_calling) is True

    # Third request should be denied
    assert await executor.request_permission(mock_api_calling) is False
```

#### 3.4.3 Test Case: RateLimitedExecutor Request Permission - Token Limits

**Purpose:** Verify that RateLimitedExecutor correctly applies token limits.

**Test Implementation:**

```python
async def test_rate_limited_executor_request_permission_token_limits(mock_api_calling):
    # Arrange
    executor = RateLimitedExecutor(
        queue_capacity=100,
        capacity_refresh_time=1.0,
        limit_requests=None,
        limit_tokens=100
    )
    mock_api_calling.required_tokens = 50

    # Act & Assert
    # First request should be granted
    assert await executor.request_permission(mock_api_calling) is True
    assert executor.available_token == 50

    # Second request should be granted
    assert await executor.request_permission(mock_api_calling) is True
    assert executor.available_token == 0

    # Third request should be denied
    assert await executor.request_permission(mock_api_calling) is False
```

### 3.5 Test Suite: Endpoint

#### 3.5.1 Test Case: Endpoint HTTP Client Creation

**Purpose:** Verify that Endpoint correctly creates and manages HTTP clients.

**Test Implementation:**

```python
def test_endpoint_create_client():
    # Arrange
    config = EndpointConfig(
        provider="test",
        endpoint="test",
        base_url="https://example.com",
        transport_type="http",
        timeout=10.0
    )
    endpoint = Endpoint(config=config)

    # Act
    client = endpoint._create_client()

    # Assert
    assert isinstance(client, aiohttp.ClientSession)
    assert client._timeout._timeout == 10.0
```

#### 3.5.2 Test Case: Endpoint Context Manager

**Purpose:** Verify that Endpoint correctly manages client lifecycle as a
context manager.

**Test Implementation:**

```python
async def test_endpoint_context_manager():
    # Arrange
    config = EndpointConfig(
        provider="test",
        endpoint="test",
        base_url="https://example.com",
        transport_type="http",
        timeout=10.0
    )
    endpoint = Endpoint(config=config)

    # Act & Assert
    async with endpoint as ep:
        assert ep.client is not None
        assert isinstance(ep.client, aiohttp.ClientSession)

    # After context exit, client should be closed
    assert endpoint.client is None
```

#### 3.5.3 Test Case: Endpoint Call Method

**Purpose:** Verify that Endpoint.call correctly uses the HTTP client.

**Setup:**

```python
@pytest.fixture
def mock_response():
    response = Mock()
    response.status = 200
    response.json = AsyncMock(return_value={"result": "success"})
    response.closed = False
    response.release = AsyncMock()
    return response

@pytest.fixture
def mock_client_session():
    session = Mock(spec=aiohttp.ClientSession)
    session.request = AsyncMock()
    return session
```

**Test Implementation:**

```python
async def test_endpoint_call(mock_client_session, mock_response):
    # Arrange
    config = EndpointConfig(
        provider="test",
        endpoint="test",
        base_url="https://example.com",
        method="POST",
        transport_type="http",
        timeout=10.0
    )
    endpoint = Endpoint(config=config)
    endpoint.client = mock_client_session
    mock_client_session.request.return_value = mock_response

    # Act
    result = await endpoint._call_aiohttp(
        payload={"param": "value"},
        headers={"Content-Type": "application/json"}
    )

    # Assert
    mock_client_session.request.assert_called_once_with(
        method="POST",
        url="https://example.com/test",
        headers={"Content-Type": "application/json"},
        json={"param": "value"}
    )
    assert result == {"result": "success"}
```

## 4. Integration Tests

### 4.1 Test Suite: APICalling with Endpoint

**Purpose:** Verify that APICalling correctly integrates with Endpoint.

**Setup:**

```python
@pytest.fixture
def mock_endpoint_with_response():
    endpoint = Mock(spec=Endpoint)
    endpoint.call = AsyncMock(return_value={"result": "success"})
    return endpoint
```

**Test Implementation:**

```python
async def test_api_calling_with_endpoint(mock_endpoint_with_response):
    # Arrange
    request = {"param": "value"}
    api_call = APICalling(
        endpoint=mock_endpoint_with_response,
        request=request,
        cache_control=True
    )

    # Act
    await api_call.invoke()

    # Assert
    mock_endpoint_with_response.call.assert_called_once_with(
        request=request,
        cache_control=True
    )
    assert api_call.execution.status == ExecutionStatus.COMPLETED
    assert api_call.execution.result == {"result": "success"}
```

### 4.2 Test Suite: Executor with Queue

**Purpose:** Verify that Executor correctly integrates with Queue.

**Test Implementation:**

```python
async def test_executor_with_queue():
    # Arrange
    executor = Executor(
        event_type=Event,
        queue_config=QueueConfig(
            queue_capacity=100,
            capacity_refresh_time=1.0
        )
    )

    # Create a mock event
    mock_event = Mock(spec=Event)
    mock_event.id = uuid.uuid4()
    mock_event.invoke = AsyncMock()
    mock_event.execution = Mock()

    # Act
    async with executor:
        executor.append(mock_event)
        await executor.forward()

        # Give the executor time to process the event
        await asyncio.sleep(0.5)

    # Assert
    mock_event.invoke.assert_called_once()
```

### 4.3 Test Suite: RateLimitedExecutor with Queue

**Purpose:** Verify that RateLimitedExecutor correctly integrates with Queue and
applies rate limits.

**Test Implementation:**

```python
async def test_rate_limited_executor_with_queue():
    # Arrange
    executor = RateLimitedExecutor(
        queue_capacity=100,
        capacity_refresh_time=1.0,
        limit_requests=2,
        limit_tokens=None
    )

    # Create mock events
    events = []
    for _ in range(3):
        event = Mock(spec=APICalling)
        event.id = uuid.uuid4()
        event.invoke = AsyncMock()
        event.execution = Mock()
        event.required_tokens = None
        events.append(event)

    # Act
    async with executor:
        for event in events:
            executor.append(event)
        await executor.forward()

        # Give the executor time to process events
        await asyncio.sleep(1.0)

    # Assert
    # Only the first two events should be invoked due to rate limiting
    events[0].invoke.assert_called_once()
    events[1].invoke.assert_called_once()
    events[2].invoke.assert_not_called()
```

### 4.4 Test Suite: Endpoint with Real HTTP Client

**Purpose:** Verify that Endpoint correctly manages a real
aiohttp.ClientSession.

**Test Implementation:**

```python
async def test_endpoint_with_real_http_client():
    # Arrange
    config = EndpointConfig(
        provider="test",
        endpoint="test",
        base_url="https://httpbin.org",
        method="GET",
        transport_type="http",
        timeout=10.0
    )
    endpoint = Endpoint(config=config)

    # Act
    async with endpoint as ep:
        # Verify client is created
        assert ep.client is not None
        assert isinstance(ep.client, aiohttp.ClientSession)

        # Make a real HTTP request
        result = await ep._call_aiohttp(payload={}, headers={})

        # Verify result
        assert isinstance(result, dict)

    # After context exit, client should be closed
    assert endpoint.client is None
```

## 5. Resource Management Tests

### 5.1 Test Suite: Endpoint Resource Cleanup

**Purpose:** Verify that Endpoint properly cleans up resources.

**Test Implementation:**

```python
async def test_endpoint_resource_cleanup():
    # Arrange
    config = EndpointConfig(
        provider="test",
        endpoint="test",
        base_url="https://example.com",
        transport_type="http",
        timeout=10.0
    )
    endpoint = Endpoint(config=config)

    # Act
    async with endpoint as ep:
        client = ep.client
        assert client is not None

        # Simulate an exception
        with pytest.raises(Exception):
            raise Exception("Test exception")

    # Assert
    # Client should still be closed even after an exception
    assert endpoint.client is None
```

### 5.2 Test Suite: Executor Resource Cleanup

**Purpose:** Verify that Executor properly cleans up resources.

**Test Implementation:**

```python
async def test_executor_resource_cleanup():
    # Arrange
    executor = Executor(
        event_type=Event,
        queue_config=QueueConfig(
            queue_capacity=100,
            capacity_refresh_time=1.0
        )
    )

    # Act
    async with executor as exe:
        # Verify task_queue is created and started
        assert exe.task_queue is not None
        assert not exe.task_queue.is_stopped()

        # Simulate an exception
        with pytest.raises(Exception):
            raise Exception("Test exception")

    # Assert
    # task_queue should be stopped and cleared even after an exception
    assert executor.task_queue is None
```

### 5.3 Test Suite: RateLimitedExecutor Resource Cleanup

**Purpose:** Verify that RateLimitedExecutor properly cleans up resources,
including the rate limit replenisher task.

**Test Implementation:**

```python
async def test_rate_limited_executor_resource_cleanup():
    # Arrange
    executor = RateLimitedExecutor(
        queue_capacity=100,
        capacity_refresh_time=1.0,
        limit_requests=10,
        limit_tokens=1000
    )

    # Act
    async with executor as exe:
        # Verify replenisher task is created
        assert exe._rate_limit_replenisher_task is not None

        # Simulate an exception
        with pytest.raises(Exception):
            raise Exception("Test exception")

    # Assert
    # Replenisher task should be cancelled even after an exception
    assert executor._rate_limit_replenisher_task is None
```

## 6. Concurrency Tests

### 6.1 Test Suite: Queue Concurrency

**Purpose:** Verify that Queue correctly handles concurrent operations.

**Test Implementation:**

```python
async def test_queue_concurrent_operations():
    # Arrange
    queue = Queue(
        queue_capacity=100,
        capacity_refresh_time=1.0,
        concurrency_limit=5
    )
    await queue.start()

    # Create mock events
    events = [Mock(spec=Event) for _ in range(10)]

    # Act
    # Concurrently enqueue all events
    enqueue_tasks = [queue.enqueue(event) for event in events]
    await asyncio.gather(*enqueue_tasks)

    # Concurrently dequeue all events
    dequeue_tasks = [queue.dequeue() for _ in range(5)]  # Only 5 due to concurrency limit
    dequeued_events = await asyncio.gather(*dequeue_tasks)

    # Assert
    assert len(dequeued_events) == 5
    for event in dequeued_events:
        assert event in events

    # Cleanup
    for _ in range(5):
        queue.task_done()
    await queue.stop()
```

### 6.2 Test Suite: Executor Concurrency

**Purpose:** Verify that Executor correctly handles concurrent event processing.

**Test Implementation:**

```python
async def test_executor_concurrent_processing():
    # Arrange
    executor = Executor(
        event_type=Event,
        queue_config=QueueConfig(
            queue_capacity=100,
            capacity_refresh_time=1.0,
            concurrency_limit=5
        )
    )

    # Create mock events with delays to simulate processing time
    events = []
    for i in range(10):
        event = Mock(spec=Event)
        event.id = uuid.uuid4()

        # Create an invoke method that sleeps for a short time
        async def invoke_with_delay(delay=0.1 * (i % 5)):
            await asyncio.sleep(delay)
            return {"result": f"success-{i}"}

        event.invoke = invoke_with_delay
        event.execution = Mock()
        events.append(event)

    # Act
    async with executor:
        for event in events:
            executor.append(event)
        await executor.forward()

        # Wait for all events to be processed
        while not executor.is_all_processed:
            await asyncio.sleep(0.1)

    # Assert
    # All events should have been processed
    for event in events:
        event.invoke.assert_called_once()
```

## 7. Mock Implementation Details

### 7.1 Mock Endpoint

```python
class MockEndpoint:
    def __init__(self, response=None, error=None):
        self.response = response or {"result": "success"}
        self.error = error
        self.calls = []
        self.config = EndpointConfig(
            provider="test",
            endpoint="test",
            base_url="https://example.com",
            method="POST",
            transport_type="http",
            timeout=10.0
        )

    async def call(self, request, cache_control=False, **kwargs):
        self.calls.append({
            "request": request,
            "cache_control": cache_control,
            "kwargs": kwargs
        })

        if self.error:
            raise self.error

        return self.response
```

### 7.2 Mock Event

```python
class MockEvent:
    def __init__(self, id_=None, result=None, error=None):
        self.id = id_ or uuid.uuid4()
        self.result = result
        self.error = error
        self.execution = Mock()
        self.execution.status = ExecutionStatus.PENDING

    async def invoke(self):
        if self.error:
            self.execution.status = ExecutionStatus.FAILED
            self.execution.error = str(self.error)
            raise self.error

        self.execution.status = ExecutionStatus.COMPLETED
        self.execution.result = self.result
        return self.result
```

## 8. Test Data

### 8.1 API Request Data

```python
test_api_requests = [
    {"param": "value1"},
    {"param": "value2", "extra": "data"},
    {"param": "value3", "nested": {"key": "value"}}
]
```

### 8.2 API Response Data

```python
test_api_responses = [
    {"result": "success", "data": {"id": "1"}},
    {"result": "success", "data": {"id": "2"}},
    {"result": "error", "error": {"code": "invalid_request"}}
]
```

## 9. Helper Functions

### 9.1 Async Context Manager Helpers

```python
@contextlib.asynccontextmanager
async def mock_aiohttp_client_context(response):
    """Create a mock aiohttp client that returns the specified response."""
    client = Mock(spec=aiohttp.ClientSession)
    client.request = AsyncMock(return_value=response)
    client.close = AsyncMock()

    try:
        yield client
    finally:
        await client.close()
```

### 9.2 Event Creation Helpers

```python
def create_test_api_calling(endpoint, request=None, requires_tokens=False, token_count=None):
    """Create a test APICalling instance with the specified parameters."""
    api_call = APICalling(
        endpoint=endpoint,
        request=request or {"param": "value"},
        requires_tokens=requires_tokens
    )

    if token_count is not None:
        api_call.required_tokens = token_count

    return api_call
```

## 10. Test Coverage Targets

- **Line Coverage Target:** 90%
- **Branch Coverage Target:** 85%
- **Critical Modules:**
  - `APICalling`: 95% coverage
  - `Endpoint` HTTP client management: 95% coverage
  - Resource cleanup code: 100% coverage

## 11. Continuous Integration

```yaml
name: Test
on: [push, pull_request]
jobs:
  tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install & Test
        run: |
          pip install -r requirements-dev.txt
          pytest --cov=src tests/ --cov-report=xml
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

## 12. Notes and Caveats

### 12.1 Known Limitations

- Some tests may be sensitive to timing, especially those involving concurrency
  and rate limiting
- Integration tests with real HTTP clients may fail if the test environment has
  network issues
- Resource cleanup tests may not catch all edge cases of resource leaks

### 12.2 Future Improvements

- Add more comprehensive concurrency tests with various scenarios
- Implement property-based testing for edge cases
- Add performance benchmarks to compare old and new implementations
- Expand test coverage for error handling scenarios
