import jax
import jax.numpy as jnp
from jax.sharding import PartitionSpec as P, NamedSharding
from jaxflow.core.auto_name import AutoNameMixin

class Variable(AutoNameMixin):
    """
    A mutable, device-aware array variable for JAXFlow layers and models.

    `Variable` represents a trainable or non-trainable tensor that supports
    automatic device placement and sharding across multiple JAX devices. 
    It provides an interface similar to `tf.Variable` or Keras `Variable`, 
    but adapted for high-performance JAX workflows, including support for 
    sharding, memory kind control, and arithmetic operations.

    Args:
        initial_value (array-like): The initial value for the variable. If not provided,
            `shape` must be specified, and the variable will be initialized to zeros.
        trainable (bool): Whether the variable is trainable (i.e., updated during optimization).
            Defaults to True.
        name (str, optional): Name for this variable. If None, an auto-generated unique name is used.
        variable_def (Any, optional): Optional metadata or definition object.
        dtype (jax.numpy.dtype, optional): Data type for the variable. If not provided,
            inferred from `initial_value` or defaults to float32.
        shape (tuple, optional): Shape of the variable. Required if `initial_value` is not given.
        device (str, optional): Device placement strategy: 'auto', 'cpu', 'gpu', or 'tpu'.
            Defaults to 'auto' (auto-selects device preference order: GPU > TPU > CPU).
        shard_devices (list or str, optional): Shard the variable across multiple devices.
            If 'auto', automatically selects all devices of the platform chosen by `device`.
            If a list, must be a list of JAX devices. If None, no sharding is performed.
        sharding (Any, optional): Full JAX sharding specification (e.g., NamedSharding).
            If provided, this overrides other device placement arguments.
        memory_kind (str, optional): Overrides the memory kind for sharded variables
            (e.g., 'pinned_host', 'device').

    Inputs:
        None directly; use in layers as a parameter container.

    Input shape:
        Specified by `shape` or inferred from `initial_value`.

    Output shape:
        Matches the input shape.

    Attributes:
        name (str): Variable name, unique in context.
        trainable (bool): Whether this variable is trainable.
        shape (tuple): Shape of the variable.
        dtype (jax.numpy.dtype): Data type of the variable.
        value (jax.Array): The actual array, placed on device(s).
        device (list): List of devices over which the variable is placed or sharded.

    Example:
        ```python
        import jax
        import jax.numpy as jnp
        from jaxflow.core.variable import Variable

        # Create a variable on the default (auto-selected) device
        w = Variable(initial_value=jnp.ones((3, 3)), name="weight")

        # Create a variable and shard it across all CPUs
        cpu_devices = [d for d in jax.devices() if d.platform == "cpu"]
        if cpu_devices:
            v_sharded = Variable(initial_value=jnp.ones((4, 4)),
                                 name="kernel",
                                 shard_devices=cpu_devices)
        
        # Assign a new value
        w.assign(jnp.zeros((3, 3)))

        # Access as a numpy array
        np_arr = w.numpy()
        ```

    Raises:
        ValueError: If shape and initial_value are incompatible, or sharding setup is invalid.

    Note:
        The variable supports direct arithmetic (e.g., `var + 1`, `var @ x`), can be
        passed as input to JAX/NumPy operations, and provides methods for device and 
        sharding introspection.

    """
    def __init__(self,
                 initial_value=None,
                 trainable=True,
                 name=None,
                 variable_def=None,
                 dtype=None,
                 shape=None,
                 device='auto',
                 shard_devices=None,
                 sharding=None,
                 memory_kind=None):
        """
        Args:
            initial_value: The initial value (Python or JAX array).
            trainable: Whether the variable is trainable.
            name: Variable name.
            variable_def: Optional variable definition metadata.
            dtype: Data type; if provided, initial_value is cast to this type.
            shape: Expected shape (required if initial_value is None).
            device: A string indicating the device ('auto', 'cpu', 'gpu', or 'tpu')
                    if no sharding is used.
            shard_devices: Controls sharding across multiple devices. It can be:
                - None: the variable is placed on a single device.
                - A list of JAX devices: the variable is sharded across these devices.
                - "auto": automatically selects all devices that match the platform
                          of the device chosen by the `device` parameter.
            sharding: A full JAX sharding specification (e.g. a NamedSharding) to use.
            memory_kind: If sharding is provided, you can override its memory kind
                         (e.g. 'pinned_host' or 'device') via with_memory_kind().
        """
        # Create initial_value if not provided.
        if initial_value is None:
            if shape is None:
                raise ValueError("Either initial_value or shape must be provided")
            dtype = dtype or jnp.float32
            initial_value = jnp.zeros(shape, dtype=dtype)
        else:
            initial_value = jnp.array(initial_value, dtype=dtype) if dtype is not None else jnp.array(initial_value)

        # Validate shape if provided.
        if shape is not None and initial_value.shape != shape:
            raise ValueError(f"Provided shape {shape} does not match initial value shape {initial_value.shape}")

        self.trainable = trainable
        self.name = self.auto_name(name)
        self.variable_def = variable_def
        self.shape = initial_value.shape
        self.dtype = initial_value.dtype

        # Option 1: Use a provided full sharding specification.
        if sharding is not None:
            if memory_kind is not None and hasattr(sharding, 'with_memory_kind'):
                sharding = sharding.with_memory_kind(memory_kind)
            self.sharding = sharding
            self.value = jax.device_put(initial_value, device=sharding)
        # Option 2: Use shard_devices for sharding across multiple devices.
        elif shard_devices is not None:
            # If shard_devices is "auto", select devices based on the chosen single device's platform.
            if isinstance(shard_devices, str) and shard_devices.lower() == "auto":
                auto_device = self._select_device(device)
                selected_devices = [d for d in jax.devices() if d.platform == auto_device.platform]
            elif isinstance(shard_devices, list):
                selected_devices = shard_devices
            else:
                raise ValueError("shard_devices must be either 'auto', a list of devices, or None")

            # Ensure the leading axis can be evenly split among the devices.
            shard_count = len(selected_devices)
            if initial_value.shape[0] % shard_count != 0:
                raise ValueError("The leading dimension of initial_value must be divisible by the number of shard devices")
            shards = jnp.split(initial_value, shard_count)
            self.value = jax.device_put_sharded(shards, devices=selected_devices)
            self._shard_devices = selected_devices  # Save for later use in the property.
        # Option 3: Single device placement.
        else:
            self._single_device = self._select_device(device)
            self.value = jax.device_put(initial_value, device=self._single_device)

    def _select_device(self, device):
        """
        Select a JAX device based on the provided device string.
        If 'auto', prefers GPU, then TPU, then CPU.
        """
        devices = jax.devices()
        if device == "auto":
            # Prefer GPU if available.
            gpu_devices = [d for d in devices if d.platform == "gpu"]
            if gpu_devices:
                return gpu_devices[0]
            # Then try TPU.
            tpu_devices = [d for d in devices if d.platform == "tpu"]
            if tpu_devices:
                return tpu_devices[0]
            # Fall back to CPU.
            cpu_devices = [d for d in devices if d.platform == "cpu"]
            if cpu_devices:
                return cpu_devices[0]
            return devices[0]
        else:
            selected = [d for d in devices if d.platform == device.lower()]
            if not selected:
                raise ValueError(f"No devices found for platform: {device}")
            return selected[0]

    @property
    def device(self):
        """
        Returns the list of devices used by the variable.
        If the variable is not sharded, returns a one-item list with the single device.
        """
        if hasattr(self, 'sharding') and self.sharding is not None:
            # If using a full sharding specification, try to extract devices from the mesh.
            try:
                return list(self.sharding.mesh.devices)
            except AttributeError:
                # Fall back to a string representation if not available.
                return [str(self.sharding)]
        elif hasattr(self, '_shard_devices'):
            return self._shard_devices
        else:
            return [self._single_device]

    def assign(self, new_value):
        """Assign a new value to the variable, preserving its device/sharding layout."""
        new_value = jnp.array(new_value, dtype=self.dtype)
        if new_value.shape != self.value.shape:
            raise ValueError("New value shape must match the variable's shape")

        if hasattr(self, 'sharding') and self.sharding is not None:
            self.value = jax.device_put(new_value, device=self.sharding)
        elif hasattr(self, '_shard_devices'):
            shard_count = len(self._shard_devices)
            if new_value.shape[0] % shard_count != 0:
                raise ValueError("The leading dimension of new value must be divisible by the number of shard devices")
            shards = jnp.split(new_value, shard_count)
            self.value = jax.device_put_sharded(shards, devices=self._shard_devices)
        else:
            self.value = jax.device_put(new_value, device=self._single_device)

    def read_value(self):
        """Return the current value (a sharded or single jax.Array)."""
        return self.value

    def numpy(self):
        """Gather and return the underlying array as a host array."""
        return jax.device_get(self.value)

    # ------------------------------------------------------------------
    # Tell JAX (and NumPy) how to turn a Variable into an array
    # ------------------------------------------------------------------
    def __jax_array__(self, dtype=None):
        """Let JAX treat Variable as a jax.Array / tracer input."""
        return self.value.astype(dtype) if dtype is not None else self.value

    def __array__(self, dtype=None):
        """Let NumPy convert Variable → np.ndarray when needed."""
        host = jax.device_get(self.value)      # device→host copy if necessary
        return host.astype(dtype) if dtype is not None else host

     # -------------- ARITHMETIC OVERLOADS --------------

    # +  and  r+
    def __add__(self, other):
        return self.value + other

    def __radd__(self, other):
        return other + self.value

    # -  and  r-
    def __sub__(self, other):
        return self.value - other

    def __rsub__(self, other):
        return other - self.value

    # *  and  r*
    def __mul__(self, other):
        return self.value * other

    def __rmul__(self, other):
        return other * self.value

    # /  and  r/
    def __truediv__(self, other):
        return self.value / other

    def __rtruediv__(self, other):
        return other / self.value

    # % (mod)  and  r%
    def __mod__(self, other):
        return self.value % other

    def __rmod__(self, other):
        return other % self.value

    # @  and  r@  (matrix multiply)
    def __matmul__(self, other):
        """
        This uses jnp.matmul under the hood if you do my_var @ other_var
        or my_var @ jnp.array(...).
        """
        return self.value @ other

    def __rmatmul__(self, other):
        """
        Called if the left operand doesn't implement __matmul__.
        """
        return other @ self.value

    # You can add more, e.g. __pow__, __floordiv__, etc.

    # -------------- ATTRIBUTE FORWARDING (Optional) --------------
    def __getattr__(self, name):
        """
        If you want methods like my_var.mean() or my_var.sum() to work,
        forward attribute lookups to the underlying jax array.
        """
        return getattr(self.value, name)

    def __repr__(self):
        full_value = jax.device_get(self.value)
        name_str = f"{self.name}:0" if self.name is not None else "Variable:0"
        if hasattr(self, 'sharding') and self.sharding is not None:
            try:
                device_list = list(self.sharding.mesh.devices)
                device_str = f"sharded over {device_list}"
            except AttributeError:
                device_str = f"sharded with {self.sharding}"
        elif hasattr(self, '_shard_devices'):
            device_str = f"sharded over {self._shard_devices}"
        else:
            device_str = str(self._single_device)
        return (f"<jaxflow.Variable '{name_str}' shape={self.shape} dtype={self.dtype}, "
                f"device={device_str}, numpy=\n{full_value}>")
"""
# Example usage:
if __name__ == "__main__":
    # Option A: Single device variable (automatically selects GPU/TPU/CPU)
    initial_val = jnp.arange(9).reshape((3, 3))
    var_single = Variable(initial_value=initial_val, name="weight", device='auto')
    print("Single-device variable:")
    print(var_single)
    print("Devices:", var_single.device)

    # Option B: Sharding using shard_devices set to "auto"
    # This will automatically select all devices that match the platform determined by 'device'.
    var_sharded_auto = Variable(initial_value=jnp.arange(16).reshape((4, 4)),
                                name="kernel",
                                device='cpu',   # or 'gpu' or 'tpu'
                                shard_devices="auto")
    print("\nVariable sharded automatically over devices of type 'cpu':")
    print(var_sharded_auto)
    print("Devices:", var_sharded_auto.device)

    # Option C: Sharding using shard_devices as a list.
    # Here we use CPU devices for demonstration (most systems have at least one).
    cpu_devices = [d for d in jax.devices() if d.platform == "cpu"]
    # Make sure the shape's leading axis is divisible by number of devices.
    if cpu_devices and (initial_val.shape[0] % len(cpu_devices) == 0):
        var_sharded_list = Variable(initial_value=jnp.arange(16).reshape((4, 4)),
                                    name="bias",
                                    shard_devices=cpu_devices)
        print("\nVariable sharded over provided device list:")
        print(var_sharded_list)
    else:
        print("\nNo suitable CPU devices found or shape mismatch for list-based sharding.")

"""