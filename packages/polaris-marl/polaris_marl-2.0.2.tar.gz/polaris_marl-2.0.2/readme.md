# POLARIS

<div align="center">

**P**artially **O**bservable **L**earning with **A**ctive **R**einforcement **I**n **S**ocial Environments

*A multi-agent reinforcement learning framework for strategic social learning*

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ’¡ Examples](#-examples) â€¢ [ğŸ”¬ Research Features](#-research-features)

</div>

---

## ğŸ¯ Overview

POLARIS is a multi-agent reinforcement learning framework for studying **strategic social learning**. It implements two canonical environments from economic theory and provides sophisticated neural architectures for modeling how agents learn from both private signals and social observations.

### ğŸ§® Theoretical Foundation

POLARIS introduces **Partially Observable Active Markov Games (POAMGs)**, extending traditional multi-agent frameworks to handle strategic learning under partial observability. Key theoretical contributions include:

- **Convergence Guarantees**: Stochastically stable distributions ensure well-defined limiting behavior
- **Policy Gradient Theorems**: Novel gradients for belief-conditioned policies in non-stationary environments  
- **Active Equilibrium Concepts**: Strategic reasoning about influencing others' learning processes

ğŸ“– **[Read the full theoretical treatment â†’](docs/thesis.pdf)**

### ğŸ† Key Features

- **ğŸ§  Theoretical Foundation**: Based on Partially Observable Active Markov Games (POAMGs)
- **ğŸ® Strategic Learning**: Agents influence others' learning processes under partial observability
- **ğŸ¤ Advanced Architectures**: Graph Neural Networks, Transformers, and Temporal Attention
- **ğŸ”„ Continual Learning**: Synaptic Intelligence prevents catastrophic forgetting
- **ğŸ“Š Two Environments**: Brandl social learning and Keller-Rady strategic experimentation

## ğŸš€ Quick Start

### Installation

```bash
# Basic installation
pip install polaris-marl

# With all features (recommended)
pip install polaris-marl[all]
```

### Command Line Usage

#### **General Purpose**
```bash
# Social learning experiment
polaris-simulate --environment-type brandl --num-agents 5 --num-states 3

# Strategic experimentation
polaris-simulate --environment-type strategic_experimentation --num-agents 4 --continuous-actions
```

#### **Research Scripts**
```bash
# Social learning with enhanced analysis
polaris-brandl --agents 8 --signal-accuracy 0.75 --plot-states --latex-style

# Strategic experimentation with allocations
polaris-strategic --agents 2 --horizon 10000 --plot-allocations --use-gnn

# Comparative analysis across agent counts
polaris-sweep --agent-counts 2 3 4 5 --horizon 400

# List all available scripts and examples
python -m polaris.experiments
```

### Python API

```python
from polaris.environments.social_learning import SocialLearningEnvironment
from polaris.training.trainer import Trainer
from polaris.config.args import parse_args

# Create environment
env = SocialLearningEnvironment(
    num_agents=5,
    num_states=3,
    signal_accuracy=0.8,
    network_type='complete'
)

# Configure and train
args = parse_args()
trainer = Trainer(env, args)
results = trainer.run_agents(training=True)
```

## ğŸ”¬ Research Features

### ğŸŒ Environments

**Brandl Social Learning**: Agents learn about a hidden state through private signals and social observation
- Discrete actions, configurable networks, theoretical bounds analysis

**Strategic Experimentation (Keller-Rady)**: Agents allocate resources between safe and risky options
- Continuous actions, LÃ©vy processes, exploration-exploitation trade-offs

### ğŸ§  Neural Architectures

- **Graph Neural Networks**: Temporal attention over social networks
- **Transformers**: Advanced belief state processing
- **Variational Inference**: Opponent modeling and belief updating

### ğŸ¯ Advanced Features

```bash
# Graph Neural Networks with temporal attention
polaris-simulate --use-gnn --gnn-layers 3 --attn-heads 8

# Continual learning with Synaptic Intelligence
polaris-simulate --use-si --si-importance 150.0

# Enhanced visualizations
polaris-brandl --plot-states --latex-style
polaris-strategic --plot-allocations --save-model
```

## ğŸ’¡ Examples

### Research Workflow
```bash
# 1. Train social learning agents
polaris-brandl --agents 8 --signal-accuracy 0.75 --use-gnn --plot-states

# 2. Strategic experimentation
polaris-strategic --agents 2 --horizon 10000 --plot-allocations --latex-style

# 3. Comparative analysis
polaris-sweep --agent-counts 2 3 4 5 6 7 8 --episodes 3
```

### Advanced Configuration
```python
from polaris.config.experiment_config import ExperimentConfig, AgentConfig, TrainingConfig

# Custom configuration
config = ExperimentConfig(
    agent=AgentConfig(use_gnn=True, use_si=True),
    training=TrainingConfig(num_episodes=10, horizon=1000)
)
```

## ğŸ“Š Console Scripts Reference

| Command | Purpose | Key Features |
|---------|---------|-------------|
| `polaris-simulate` | General experimentation | Flexible, all environments |
| `polaris-brandl` | Social learning research | Theoretical bounds, belief analysis |
| `polaris-strategic` | Strategic experimentation | Allocation plots, KL divergence |
| `polaris-sweep` | Multi-agent comparison | Statistical analysis, confidence intervals |
| `polaris-experiment` | Quick testing | Simplified interface |

## ğŸ› ï¸ Development

```bash
# Development installation
git clone https://github.com/ecdogaroglu/polaris.git
cd polaris
pip install -e .[all]

# Run tests
pytest tests/

# Check available experiments
python -m polaris.experiments
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ“š Citation

```bibtex
@software{polaris2025,
  title={POLARIS: Partially Observable Learning with Active Reinforcement In Social Environments},
  author={Ege Can DoÄŸaroÄŸlu},
  year={2025},
  url={https://github.com/ecdogaroglu/polaris}
}
```

---

<div align="center">

[â­ Star on GitHub](https://github.com/ecdogaroglu/polaris) â€¢ [ğŸ› Report Issues](https://github.com/ecdogaroglu/polaris/issues)

</div> 