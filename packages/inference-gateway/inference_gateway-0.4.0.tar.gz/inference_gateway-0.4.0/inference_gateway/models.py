# generated by datamodel-codegen:
#   filename:  openapi.yaml
#   timestamp: 2025-05-26T15:46:03+00:00

from __future__ import annotations

from collections.abc import Mapping, Sequence
from typing import Annotated, Any, Literal, Optional

from pydantic import BaseModel, ConfigDict, Field, RootModel


class Provider(
    RootModel[Literal["ollama", "groq", "openai", "cloudflare", "cohere", "anthropic", "deepseek"]]
):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    root: Literal["ollama", "groq", "openai", "cloudflare", "cohere", "anthropic", "deepseek"]

    def __eq__(self, other: Any) -> bool:
        """Allow comparison with strings."""
        if isinstance(other, str):
            return self.root == other
        return super().__eq__(other)


class ProviderSpecificResponse(BaseModel):
    """
    Provider-specific response format. Examples:

    OpenAI GET /v1/models?provider=openai response:
    ```json
    {
      "provider": "openai",
      "object": "list",
      "data": [
        {
          "id": "gpt-4",
          "object": "model",
          "created": 1687882410,
          "owned_by": "openai",
          "served_by": "openai"
        }
      ]
    }
    ```

    Anthropic GET /v1/models?provider=anthropic response:
    ```json
    {
      "provider": "anthropic",
      "object": "list",
      "data": [
        {
          "id": "gpt-4",
          "object": "model",
          "created": 1687882410,
          "owned_by": "openai",
          "served_by": "openai"
        }
      ]
    }
    ```

    """

    model_config = ConfigDict(
        populate_by_name=True,
    )


class ProviderAuthType(RootModel[Literal["bearer", "xheader", "query", "none"]]):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    root: Literal["bearer", "xheader", "query", "none"]
    """
    Authentication type for providers
    """


class SSEvent(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    event: Optional[
        Literal[
            "message-start",
            "stream-start",
            "content-start",
            "content-delta",
            "content-end",
            "message-end",
            "stream-end",
        ]
    ]
    data: Optional[str]
    retry: Optional[int]


class Endpoints(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    models: str
    chat: str


class Error(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    error: Optional[str]


class MessageRole(RootModel[Literal["system", "user", "assistant", "tool"]]):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    root: Literal["system", "user", "assistant", "tool"]
    """
    Role of the message sender
    """

    def __eq__(self, other: Any) -> bool:
        """Allow comparison with strings."""
        if isinstance(other, str):
            return self.root == other
        return super().__eq__(other)


class Model(BaseModel):
    """
    Common model information
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    id: str
    object: str
    created: int
    owned_by: str
    served_by: Provider


class ListModelsResponse(BaseModel):
    """
    Response structure for listing models
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    provider: Optional[Provider]
    object: str
    data: Sequence[Model]


class MCPTool(BaseModel):
    """
    An MCP tool definition
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    name: Annotated[str, Field(examples=["read_file"])]
    """
    The name of the tool
    """
    description: Annotated[str, Field(examples=["Read content from a file"])]
    """
    A description of what the tool does
    """
    server: Annotated[str, Field(examples=["http://mcp-filesystem-server:8083/mcp"])]
    """
    The MCP server that provides this tool
    """
    input_schema: Annotated[
        Optional[Mapping[str, Any]],
        Field(
            examples=[
                {
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string", "description": "Path to the file to read"}
                    },
                    "required": ["file_path"],
                }
            ]
        ),
    ]
    """
    JSON schema for the tool's input parameters
    """


class FunctionParameters(BaseModel):
    """
    The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.
    Omitting `parameters` defines a function with an empty parameter list.
    """

    model_config = ConfigDict(
        extra="allow",
        populate_by_name=True,
    )


class ChatCompletionToolType(RootModel[Literal["function"]]):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    root: Literal["function"] = "function"
    """
    The type of the tool. Currently, only `function` is supported.
    """


class CompletionUsage(BaseModel):
    """
    Usage statistics for the completion request.
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    completion_tokens: int
    """
    Number of tokens in the generated completion.
    """
    prompt_tokens: int
    """
    Number of tokens in the prompt.
    """
    total_tokens: int
    """
    Total number of tokens used in the request (prompt + completion).
    """


class ChatCompletionStreamOptions(BaseModel):
    """
    Options for streaming response. Only set this when you set `stream: true`.

    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    include_usage: bool
    """
    If set, an additional chunk will be streamed before the `data: [DONE]` message. The `usage` field on this chunk shows the token usage statistics for the entire request, and the `choices` field will always be an empty array. All other chunks will also include a `usage` field, but with a null value.

    """


class ChatCompletionMessageToolCallFunction(BaseModel):
    """
    The function that the model called.
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    name: str
    """
    The name of the function to call.
    """
    arguments: str
    """
    The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    """


class ChatCompletionMessageToolCall(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    id: str
    """
    The ID of the tool call.
    """
    type: ChatCompletionToolType
    function: ChatCompletionMessageToolCallFunction


class Function(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    name: Optional[str]
    """
    The name of the function to call.
    """
    arguments: Optional[str]
    """
    The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    """


class ChatCompletionMessageToolCallChunk(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    index: int
    id: Optional[str]
    """
    The ID of the tool call.
    """
    type: Optional[str]
    """
    The type of the tool. Currently, only `function` is supported.
    """
    function: Optional[Function]


class TopLogprob(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    token: str
    """
    The token.
    """
    logprob: float
    """
    The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    """
    bytes: Sequence[int]
    """
    A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
    """


class ChatCompletionTokenLogprob(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    token: str
    """
    The token.
    """
    logprob: float
    """
    The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    """
    bytes: Sequence[int]
    """
    A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
    """
    top_logprobs: Sequence[TopLogprob]
    """
    List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned.
    """


class FinishReason(
    RootModel[Literal["stop", "length", "tool_calls", "content_filter", "function_call"]]
):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    root: Literal["stop", "length", "tool_calls", "content_filter", "function_call"]
    """
    The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
    `length` if the maximum number of tokens specified in the request was reached,
    `content_filter` if content was omitted due to a flag from our content filters,
    `tool_calls` if the model called a tool.

    """


class Config(RootModel[Any]):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    root: Any


class Message(BaseModel):
    """
    Message structure for provider requests
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    role: MessageRole
    content: str
    tool_calls: Optional[Sequence[ChatCompletionMessageToolCall]] = None
    tool_call_id: Optional[str] = None
    reasoning_content: Optional[str] = None
    """
    The reasoning content of the chunk message.
    """
    reasoning: Optional[str] = None
    """
    The reasoning of the chunk message. Same as reasoning_content.
    """


class ListToolsResponse(BaseModel):
    """
    Response structure for listing MCP tools
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    object: Annotated[str, Field(examples=["list"])]
    """
    Always "list"
    """
    data: Sequence[MCPTool]
    """
    Array of available MCP tools
    """


class FunctionObject(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    description: Optional[str]
    """
    A description of what the function does, used by the model to choose when and how to call the function.
    """
    name: str
    """
    The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
    """
    parameters: Optional[FunctionParameters]
    strict: bool = False
    """
    Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Learn more about Structured Outputs in the [function calling guide](docs/guides/function-calling).
    """


class ChatCompletionTool(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    type: ChatCompletionToolType
    function: FunctionObject


class CreateChatCompletionRequest(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    model: str
    """
    Model ID to use
    """
    messages: Annotated[Sequence[Message], Field(min_length=1)]
    """
    A list of messages comprising the conversation so far.

    """
    max_tokens: Optional[int] = None
    """
    An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.

    """
    stream: bool = False
    """
    If set to true, the model response data will be streamed to the client as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).

    """
    stream_options: Optional[ChatCompletionStreamOptions] = None
    tools: Optional[Sequence[ChatCompletionTool]] = None
    """
    A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.

    """
    reasoning_format: Optional[str] = None
    """
    The format of the reasoning content. Can be `raw` or `parsed`.
    When specified as raw some reasoning models will output <think /> tags. When specified as parsed the model will output the reasoning under  `reasoning` or `reasoning_content` attribute.

    """


class ChatCompletionChoice(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    finish_reason: Literal["stop", "length", "tool_calls", "content_filter", "function_call"]
    """
    The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
    `length` if the maximum number of tokens specified in the request was reached,
    `content_filter` if content was omitted due to a flag from our content filters,
    `tool_calls` if the model called a tool.

    """
    index: int
    """
    The index of the choice in the list of choices.
    """
    message: Message


class Logprobs(BaseModel):
    """
    Log probability information for the choice.
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    content: Sequence[ChatCompletionTokenLogprob]
    """
    A list of message content tokens with log probability information.
    """
    refusal: Sequence[ChatCompletionTokenLogprob]
    """
    A list of message refusal tokens with log probability information.
    """


class CreateChatCompletionResponse(BaseModel):
    """
    Represents a chat completion response returned by model, based on the provided input.
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    id: str
    """
    A unique identifier for the chat completion.
    """
    choices: Sequence[ChatCompletionChoice]
    """
    A list of chat completion choices. Can be more than one if `n` is greater than 1.
    """
    created: int
    """
    The Unix timestamp (in seconds) of when the chat completion was created.
    """
    model: str
    """
    The model used for the chat completion.
    """
    object: str
    """
    The object type, which is always `chat.completion`.
    """
    usage: Optional[CompletionUsage]


class ChatCompletionStreamResponseDelta(BaseModel):
    """
    A chat completion delta generated by streamed model responses.
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    content: str
    """
    The contents of the chunk message.
    """
    reasoning_content: Optional[str]
    """
    The reasoning content of the chunk message.
    """
    reasoning: Optional[str]
    """
    The reasoning of the chunk message. Same as reasoning_content.
    """
    tool_calls: Optional[Sequence[ChatCompletionMessageToolCallChunk]]
    role: MessageRole
    refusal: Optional[str]
    """
    The refusal message generated by the model.
    """


class ChatCompletionStreamChoice(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    delta: ChatCompletionStreamResponseDelta
    logprobs: Optional[Logprobs]
    """
    Log probability information for the choice.
    """
    finish_reason: FinishReason
    index: int
    """
    The index of the choice in the list of choices.
    """


class CreateChatCompletionStreamResponse(BaseModel):
    """
    Represents a streamed chunk of a chat completion response returned
    by the model, based on the provided input.

    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    id: str
    """
    A unique identifier for the chat completion. Each chunk has the same ID.
    """
    choices: Sequence[ChatCompletionStreamChoice]
    """
    A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the
    last chunk if you set `stream_options: {"include_usage": true}`.

    """
    created: int
    """
    The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
    """
    model: str
    """
    The model to generate the completion.
    """
    system_fingerprint: Optional[str]
    """
    This fingerprint represents the backend configuration that the model runs with.
    Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.

    """
    object: str
    """
    The object type, which is always `chat.completion.chunk`.
    """
    usage: Optional[CompletionUsage]
    reasoning_format: Optional[str]
    """
    The format of the reasoning content. Can be `raw` or `parsed`.
    When specified as raw some reasoning models will output <think /> tags. When specified as parsed the model will output the reasoning under reasoning_content.

    """
