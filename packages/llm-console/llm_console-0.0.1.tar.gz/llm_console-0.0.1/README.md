<p align="right">
<a href="https://pypi.org/project/llm-cli/" target="_blank"><img src="https://badge.fury.io/py/llm-cli.svg" alt="PYPI Release"></a>
<a href="https://github.com/Nayjest/llm-cli/actions/workflows/code-style.yml" target="_blank"><img src="https://github.com/Nayjest/llm-cli/actions/workflows/code-style.yml/badge.svg" alt="Pylint"></a>
<a href="https://github.com/Nayjest/llm-cli/actions/workflows/tests.yml" target="_blank"><img src="https://github.com/Nayjest/llm-cli/actions/workflows/tests.yml/badge.svg" alt="Tests"></a>
<a href="https://github.com/Nayjest/llm-cli/blob/main/LICENSE" target="_blank"><img src="https://img.shields.io/static/v1?label=license&message=MIT&color=d08aff" alt="License"></a>
</p>

# ğŸ¤– Command-line interface for LLMs

## âœ¨ Features

- @todo
- Flexible configuration via [`.env` file](.env.example)
- Extremely fast, parallel LLM usage
- Model-agnostic (OpenAI, Anthropic, Google, local PyTorch inference, etc.)


## ğŸš€ Quickstart
### 1. Install the package

```bash
pip install llm-cli
```

## ğŸ¤ Contributing

We â¤ï¸ contributions! See [CONTRIBUTING.md](CONTRIBUTING.md).

## ğŸ“ License

Licensed under the [MIT License](LICENSE).

Â© 2022&mdash;2025 [Vitalii Stepanenko](mailto:mail@vitaliy.in)
