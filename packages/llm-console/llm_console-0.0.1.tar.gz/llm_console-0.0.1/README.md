<p align="right">
<a href="https://pypi.org/project/llm-cli/" target="_blank"><img src="https://badge.fury.io/py/llm-cli.svg" alt="PYPI Release"></a>
<a href="https://github.com/Nayjest/llm-cli/actions/workflows/code-style.yml" target="_blank"><img src="https://github.com/Nayjest/llm-cli/actions/workflows/code-style.yml/badge.svg" alt="Pylint"></a>
<a href="https://github.com/Nayjest/llm-cli/actions/workflows/tests.yml" target="_blank"><img src="https://github.com/Nayjest/llm-cli/actions/workflows/tests.yml/badge.svg" alt="Tests"></a>
<a href="https://github.com/Nayjest/llm-cli/blob/main/LICENSE" target="_blank"><img src="https://img.shields.io/static/v1?label=license&message=MIT&color=d08aff" alt="License"></a>
</p>

# 🤖 Command-line interface for LLMs

## ✨ Features

- @todo
- Flexible configuration via [`.env` file](.env.example)
- Extremely fast, parallel LLM usage
- Model-agnostic (OpenAI, Anthropic, Google, local PyTorch inference, etc.)


## 🚀 Quickstart
### 1. Install the package

```bash
pip install llm-cli
```

## 🤝 Contributing

We ❤️ contributions! See [CONTRIBUTING.md](CONTRIBUTING.md).

## 📝 License

Licensed under the [MIT License](LICENSE).

© 2022&mdash;2025 [Vitalii Stepanenko](mailto:mail@vitaliy.in)
