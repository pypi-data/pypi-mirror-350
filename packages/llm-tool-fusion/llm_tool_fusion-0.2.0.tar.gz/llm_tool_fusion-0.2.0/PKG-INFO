Metadata-Version: 2.4
Name: llm-tool-fusion
Version: 0.2.0
Summary: Biblioteca Python que simplifica e unifica a defini√ß√£o e chamada de ferramentas para grandes modelos de linguagem (LLMs). Compat√≠vel com Ollama, LangChain, OpenAI e outros frameworks.
Home-page: https://github.com/caua1503/llm-tool-fusion
Author: Caua ramos
Author-email: cauamedinax@gmail.com
Requires-Python: >=3.12
Description-Content-Type: text/markdown
License-File: LICENSE
Dynamic: author
Dynamic: author-email
Dynamic: home-page
Dynamic: license-file

# llm-tool-fusion

[![Python](https://img.shields.io/badge/python->=3.12-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Version](https://img.shields.io/badge/version-0.1.0-orange.svg)](pyproject.toml)

## üáßüá∑ Portugu√™s

### üìñ Descri√ß√£o

**llm-tool-fusion** √© uma biblioteca Python que simplifica e unifica a defini√ß√£o e chamada de ferramentas para grandes modelos de linguagem (LLMs). Compat√≠vel com frameworks populares que suportam tool calling, como Ollama, LangChain e OpenAI, ela permite integrar facilmente novas fun√ß√µes e m√≥dulos, tornando o desenvolvimento de aplicativos avan√ßados de IA mais √°gil e modular atraves de decoradores de fun√ß√µes.

### ‚ú® Principais Recursos

- üîß **Unifica√ß√£o de APIs**: Interface √∫nica para diferentes frameworks de LLM
- üöÄ **Integra√ß√£o Simplificada**: Adicione novas ferramentas com facilidade
- üîó **Compatibilidade Ampla**: Suporte para Ollama, LangChain, OpenAI e outros
- üì¶ **Modularidade**: Arquitetura modular para desenvolvimento escal√°vel
- ‚ö° **Performance**: Otimizado para aplica√ß√µes em produ√ß√£o
- üìù **Menos Verbosidade**: Sintaxe simplificada para declara√ß√£o de fun√ß√µes
- üîÑ **Processamento Autom√°tico**: Execu√ß√£o autom√°tica de chamadas de ferramentas (opcional)

### üöÄ Instala√ß√£o

```bash
pip install llm-tool-fusion
```

### üìã Uso B√°sico (Exemplo com OpenAI)

```python
from openai import OpenAI
from llm_tool_fusion import ToolCaller, process_tool_calls

# Inicializa o cliente OpenAI e o gerenciador de ferramentas
client = OpenAI()
manager = ToolCaller()

# Define uma ferramenta usando o decorador
@manager.tool
def calculate_price(price: float, discount: float) -> float:
    """
    Calcula o pre√ßo final com desconto
    
    Args:
        price (float): Pre√ßo base
        discount (float): Percentual de desconto
        
    Returns:
        float: Pre√ßo final com desconto
    """
    return price * (1 - discount / 100)

# Prepara a mensagem e faz a chamada ao LLM
messages = [
    {"role": "user", "content": "Calcule o pre√ßo final de um produto de R$100 com 20% de desconto"}
]

# Primeira chamada ao LLM
response = client.chat.completions.create(
    model="gpt-4",
    messages=messages,
    tools=manager.get_tools()
)

# Define a fun√ß√£o de chamada da IA
# Esta fun√ß√£o lambda permite compatibilidade com diferentes bibliotecas (OpenAI, Anthropic, etc)
# Voc√™ pode adaptar esta fun√ß√£o para usar qualquer biblioteca que desejar
llm_call_fn = lambda model, messages, tools: client.chat.completions.create(
    model=model, 
    messages=messages, 
    tools=tools
)

# Processa a resposta automaticamente
final_response = process_tool_calls(
    response=response,
    messages=messages,
    async_tools_name=manager.get_name_async_tools(),
    available_tools=manager.get_map_tools(),
    model="gpt-4",
    llm_call_fn=llm_call_fn,
    tools=manager.get_tools(),
    verbose=True,
    verbose_time=True,
    clean_messages=True
)

print(final_response)
```

### üîÑ Processamento de Chamadas

O llm-tool-fusion oferece um sistema robusto e simples para processar chamadas de ferramentas:

```python
# Processamento autom√°tico de chamadas
final_response = process_tool_calls(
    response=response,        # Resposta inicial do LLM
    messages=messages,        # Hist√≥rico de mensagens
    async_tools_name=manager.get_name_async_tools(),  # Nome das ferramentas ass√≠ncronas
    available_tools=manager.get_map_tools(),          # Mapa de ferramentas dispon√≠veis
    model="gpt-4",           # Modelo a ser usado
    llm_call_fn=llm_call_fn, # Fun√ß√£o de chamada ao LLM
    tools=manager.get_tools(),# Lista de ferramentas
    verbose=True,            # (opcional) Logs detalhados
    verbose_time=True,       # (opcional) M√©tricas de tempo
    clean_messages=True      # (opcional) Limpa mensagens ap√≥s processamento, nao e necessario .choices[0].message.content
)
```

#### ‚ú® Caracter√≠sticas Principais

- üîÅ **Ciclo Autom√°tico**: Processa todas as chamadas de ferramentas at√© a conclus√£o
- ‚ö° **Suporte Ass√≠ncrono**: Executa ferramentas s√≠ncronas e ass√≠ncronas automaticamente
- üìù **Logs Inteligentes**: Acompanhe a execu√ß√£o com logs detalhados e m√©tricas de tempo
- üõ°Ô∏è **Tratamento de Erros**: Gerenciamento robusto de erros durante a execu√ß√£o
- üí¨ **Gest√£o de Contexto**: Mant√©m o hist√≥rico de conversas organizado
- üîß **Configur√°vel**: Personalize o comportamento conforme sua necessidade

#### üöÄ Vers√£o Ass√≠ncrona

Para aplica√ß√µes que precisam de processamento ass√≠ncrono:

```python
# Processamento ass√≠ncrono de chamadas
final_response = await process_tool_calls_async(
    response=response,
    messages=messages,
    # ... mesmos par√¢metros da vers√£o s√≠ncrona ...
)
```

### üîß Frameworks Suportados

- **OpenAI** - API oficial e modelos GPT
- **LangChain** - Framework completo para aplica√ß√µes LLM
- **Ollama** - Execu√ß√£o local de modelos
- **Anthropic Claude** - API da Anthropic
- **E muito mais...**

### ü§ù Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Por favor:

1. Fa√ßa um fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### üìÑ Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

---

## üá∫üá∏ English

### üìñ Description

**llm-tool-fusion** is a Python library that simplifies and unifies the definition and calling of tools for large language models (LLMs). Compatible with popular frameworks that support tool calling, such as Ollama, LangChain, and OpenAI, it allows you to easily integrate new functions and modules, making the development of advanced AI applications more agile and modular through function decorators.

### ‚ú® Key Features

- üîß **API Unification**: Single interface for different LLM frameworks
- üöÄ **Simplified Integration**: Add new tools with ease
- üîó **Wide Compatibility**: Support for Ollama, LangChain, OpenAI, and others
- üì¶ **Modularity**: Modular architecture for scalable development
- ‚ö° **Performance**: Optimized for production applications
- üìù **Less Verbosity**: Simplified syntax for function declarations
- üîÑ **Automatic Processing**: Automatic execution of tool calls (optional)

### üöÄ Installation

```bash
pip install llm-tool-fusion
```

### üìã Basic Usage (Example with OpenAI)

```python
from openai import OpenAI
from llm_tool_fusion import ToolCaller, process_tool_calls

# Initialize OpenAI client and tool manager
client = OpenAI()
manager = ToolCaller()

# Define a tool using the decorator
@manager.tool
def calculate_price(price: float, discount: float) -> float:
    """
    Calculate final price with discount
    
    Args:
        price (float): Base price
        discount (float): Discount percentage
        
    Returns:
        float: Final price with discount
    """
    return price * (1 - discount / 100)

# Prepare message and make LLM call
messages = [
    {"role": "user", "content": "Calculate the final price of a $100 product with 20% discount"}
]

# First LLM call
response = client.chat.completions.create(
    model="gpt-4",
    messages=messages,
    tools=manager.get_tools()
)

# Define the AI call function
# This lambda function allows compatibility with different libraries (OpenAI, Anthropic, etc)
# You can adapt this function to use any library you want
llm_call_fn = lambda model, messages, tools: client.chat.completions.create(
    model=model, 
    messages=messages, 
    tools=tools
)

# Process response automatically
final_response = process_tool_calls(
    response=response,
    messages=messages,
    async_tools_name=manager.get_name_async_tools(),
    available_tools=manager.get_map_tools(),
    model="gpt-4",
    llm_call_fn=llm_call_fn,
    tools=manager.get_tools(),
    verbose=True,
    verbose_time=True,
    clean_messages=True
)

print(final_response)
```

### üîÑ Processamento de Chamadas

llm-tool-fusion provides a robust and simple system for processing tool calls:

```python
# Automatic tool call processing
final_response = process_tool_calls(
    response=response,        # Initial response from the LLM
    messages=messages,        # Message history
    async_tools_name=manager.get_name_async_tools(),  # Names of asynchronous tools
    available_tools=manager.get_map_tools(),          # Map of available tools
    model="gpt-4",           # Model to be used
    llm_call_fn=llm_call_fn, # Function to call the LLM
    tools=manager.get_tools(),# List of tools
    verbose=True,            # (optional) Detailed logs
    verbose_time=True,       # (optional) Time metrics
    clean_messages=True      # (optional) Clears messages after processing, no .choices[0].message.content required
)
```

#### ‚ú® Main Features

- üîÅ **Automatic Loop**: Processes all tool calls to completion
- ‚ö° **Asynchronous Support**: Runs synchronous and asynchronous tools automatically
- üìù **Smart Logs**: Track execution with detailed logs and time metrics
- üõ°Ô∏è **Error Handling**: Robust error management during execution
- üí¨ **Context Management**: Keeps conversation history organized
- üîß **Configurable**: Customize behavior to your needs

#### üöÄ Vers√£o Ass√≠ncrona

For applications that need asynchronous processing:

```python
# Asynchronous processing of tool calls
final_response = await process_tool_calls_async(
    response=response,
    messages=messages,
    # ... same parameters as the synchronous version ...
)
```

### üîß Supported Frameworks

- **OpenAI** - Official API and GPT models
- **LangChain** - Complete framework for LLM applications
- **Ollama** - Local model execution
- **Anthropic Claude** - Anthropic's API
- **And many more...**

### ü§ù Contributing

Contributions are welcome! Please:

1. Fork the project
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üõ†Ô∏è Development

### Prerequisites

- Python >= 3.12
- pip or poetry for dependency management

### Setup Development Environment

```bash
# Clone the repository
git clone https://github.com/caua1503/llm-tool-fusion.git
cd llm-tool-fusion

# Install dependencies
pip install -e .

# Run tests
python -m pytest
```

### Project Structure

```
llm-tool-fusion/
‚îú‚îÄ‚îÄ llm_tool_fusion/
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
|   ‚îî‚îÄ‚îÄ _core.py
|   ‚îî‚îÄ‚îÄ _utils.py
‚îÇ      
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ examples/
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ README.md
```

---

**‚≠ê Se este projeto foi √∫til para voc√™, considere dar uma estrela no GitHub!**

**‚≠ê If this project was helpful to you, consider starring it on GitHub!**
