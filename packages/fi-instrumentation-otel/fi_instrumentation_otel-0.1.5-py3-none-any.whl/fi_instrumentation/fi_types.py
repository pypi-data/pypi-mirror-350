from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Type

from fi_instrumentation.settings import get_env_collector_endpoint


class SpanAttributes:
    OUTPUT_VALUE = "output.value"
    OUTPUT_MIME_TYPE = "output.mime_type"
    """
    The type of output.value. If unspecified, the type is plain text by default.
    If type is JSON, the value is a string representing a JSON object.
    """
    INPUT_VALUE = "input.value"
    INPUT_MIME_TYPE = "input.mime_type"
    """
    The type of input.value. If unspecified, the type is plain text by default.
    If type is JSON, the value is a string representing a JSON object.
    """

    EMBEDDING_EMBEDDINGS = "embedding.embeddings"
    """
    A list of objects containing embedding data, including the vector and represented piece of text.
    """
    EMBEDDING_MODEL_NAME = "embedding.model_name"
    """
    The name of the embedding model.
    """

    LLM_FUNCTION_CALL = "llm.function_call"
    """
    For models and APIs that support function calling. Records attributes such as the function
    name and arguments to the called function.
    """
    LLM_INVOCATION_PARAMETERS = "llm.invocation_parameters"
    """
    Invocation parameters passed to the LLM or API, such as the model name, temperature, etc.
    """
    LLM_INPUT_MESSAGES = "llm.input_messages"
    """
    Messages provided to a chat API.
    """
    LLM_OUTPUT_MESSAGES = "llm.output_messages"
    """
    Messages received from a chat API.
    """
    LLM_MODEL_NAME = "llm.model_name"
    """
    The name of the model being used.
    """
    LLM_PROVIDER = "llm.provider"
    """
    The provider of the model, such as OpenAI, Azure, Google, etc.
    """
    LLM_SYSTEM = "llm.system"
    """
    The AI product as identified by the client or server
    """
    LLM_PROMPTS = "llm.prompts"
    """
    Prompts provided to a completions API.
    """
    LLM_PROMPT_TEMPLATE = "llm.prompt_template.template"
    """
    The prompt template as a Python f-string.
    """
    LLM_PROMPT_TEMPLATE_VARIABLES = "llm.prompt_template.variables"
    """
    A list of input variables to the prompt template.
    """
    LLM_PROMPT_TEMPLATE_VERSION = "llm.prompt_template.version"
    """
    The version of the prompt template being used.
    """
    LLM_TOKEN_COUNT_PROMPT = "llm.token_count.prompt"
    """
    Number of tokens in the prompt.
    """
    LLM_TOKEN_COUNT_COMPLETION = "llm.token_count.completion"
    """
    Number of tokens in the completion.
    """
    LLM_TOKEN_COUNT_TOTAL = "llm.token_count.total"
    """
    Total number of tokens, including both prompt and completion.
    """

    LLM_TOOLS = "llm.tools"
    """
    List of tools that are advertised to the LLM to be able to call
    """

    TOOL_NAME = "tool.name"
    """
    Name of the tool being used.
    """
    TOOL_DESCRIPTION = "tool.description"
    """
    Description of the tool's purpose, typically used to select the tool.
    """
    TOOL_PARAMETERS = "tool.parameters"
    """
    Parameters of the tool represented a dictionary JSON string, e.g.
    see https://platform.openai.com/docs/guides/gpt/function-calling
    """

    RETRIEVAL_DOCUMENTS = "retrieval.documents"

    METADATA = "metadata"
    """
    Metadata attributes are used to store user-defined key-value pairs.
    For example, LangChain uses metadata to store user-defined attributes for a chain.
    """

    TAG_TAGS = "tag.tags"
    """
    Custom categorical tags for the span.
    """

    FI_SPAN_KIND = "fi.span.kind"

    SESSION_ID = "session.id"
    """
    The id of the session
    """
    USER_ID = "user.id"
    """
    The id of the user
    """
    INPUT_IMAGES = "llm.input.images"
    """
    A list of input images provided to the model.
    """
    EVAL_INPUT = "eval.input"
    """
    Input being sent to the eval
    """
    RAW_INPUT = "raw.input"
    """
    Raw input being sent to otel
    """
    RAW_OUTPUT = "raw.output"
    """
    Raw output being sent from otel
    """
    QUERY = "query"
    """
    The query being sent to the model
    """
    RESPONSE = "response"
    """
    The response being sent from the model
    """


class MessageAttributes:
    """
    Attributes for a message sent to or from an LLM
    """

    MESSAGE_ROLE = "message.role"
    """
    The role of the message, such as "user", "agent", "function".
    """
    MESSAGE_CONTENT = "message.content"
    """
    The content of the message to or from the llm, must be a string.
    """
    MESSAGE_CONTENTS = "message.contents"
    """
    The message contents to the llm, it is an array of
    `message_content` prefixed attributes.
    """
    MESSAGE_NAME = "message.name"
    """
    The name of the message, often used to identify the function
    that was used to generate the message.
    """
    MESSAGE_TOOL_CALLS = "message.tool_calls"
    """
    The tool calls generated by the model, such as function calls.
    """
    MESSAGE_FUNCTION_CALL_NAME = "message.function_call_name"
    """
    The function name that is a part of the message list.
    This is populated for role 'function' or 'agent' as a mechanism to identify
    the function that was called during the execution of a tool.
    """
    MESSAGE_FUNCTION_CALL_ARGUMENTS_JSON = "message.function_call_arguments_json"
    """
    The JSON string representing the arguments passed to the function
    during a function call.
    """
    MESSAGE_TOOL_CALL_ID = "message.tool_call_id"
    """
    The id of the tool call.
    """


class MessageContentAttributes:
    """
    Attributes for the contents of user messages sent to an LLM.
    """

    MESSAGE_CONTENT_TYPE = "message_content.type"
    """
    The type of the content, such as "text" or "image" or "audio" or "video".
    """

    MESSAGE_CONTENT_TEXT = "message_content.text"
    """
    The text content of the message, if the type is "text".
    """
    MESSAGE_CONTENT_IMAGE = "message_content.image"
    """
    The image content of the message, if the type is "image".
    An image can be made available to the model by passing a link to
    the image or by passing the base64 encoded image directly in the
    request.
    """
    MESSAGE_CONTENT_AUDIO = "message_content.audio"
    """
    The audio content of the message, if the type is "audio".
    An audio file can be made available to the model by passing a link to
    the audio file or by passing the base64 encoded audio directly in the
    request.
    """
    MESSAGE_AUDIO_TRANSCRIPT = "message_content.audio.transcript"
    """
    Represents the transcript of the audio content in the message.
    """
    MESSAGE_CONTENT_VIDEO = "message_content.video"
    """
    The video content of the message, if the type is "video".
    """


class ImageAttributes:
    """
    Attributes for images
    """

    IMAGE_URL = "image.url"
    """
    An http or base64 image url
    """


class AudioAttributes:
    """
    Attributes for audio
    """

    AUDIO_URL = "audio.url"
    """
    The url to an audio file
    """
    AUDIO_MIME_TYPE = "audio.mime_type"
    """
    The mime type of the audio file
    """
    AUDIO_TRANSCRIPT = "audio.transcript"
    """
    The transcript of the audio file
    """


class DocumentAttributes:
    """
    Attributes for a document.
    """

    DOCUMENT_ID = "document.id"
    """
    The id of the document.
    """
    DOCUMENT_SCORE = "document.score"
    """
    The score of the document
    """
    DOCUMENT_CONTENT = "document.content"
    """
    The content of the document.
    """
    DOCUMENT_METADATA = "document.metadata"
    """
    The metadata of the document represented as a dictionary
    JSON string, e.g. `"{ 'title': 'foo' }"`
    """


class RerankerAttributes:
    """
    Attributes for a reranker
    """

    RERANKER_INPUT_DOCUMENTS = "reranker.input_documents"
    """
    List of documents as input to the reranker
    """
    RERANKER_OUTPUT_DOCUMENTS = "reranker.output_documents"
    """
    List of documents as output from the reranker
    """
    RERANKER_QUERY = "reranker.query"
    """
    Query string for the reranker
    """
    RERANKER_MODEL_NAME = "reranker.model_name"
    """
    Model name of the reranker
    """
    RERANKER_TOP_K = "reranker.top_k"
    """
    Top K parameter of the reranker
    """


class EmbeddingAttributes:
    """
    Attributes for an embedding
    """

    EMBEDDING_TEXT = "embedding.text"
    """
    The text represented by the embedding.
    """
    EMBEDDING_VECTOR = "embedding.vector"
    """
    The embedding vector.
    """


class ToolCallAttributes:
    """
    Attributes for a tool call
    """

    TOOL_CALL_ID = "tool_call.id"
    """
    The id of the tool call.
    """
    TOOL_CALL_FUNCTION_NAME = "tool_call.function.name"
    """
    The name of function that is being called during a tool call.
    """
    TOOL_CALL_FUNCTION_ARGUMENTS_JSON = "tool_call.function.arguments"
    """
    The JSON string representing the arguments passed to the function
    during a tool call.
    """


class ToolAttributes:
    """
    Attributes for a tools
    """

    TOOL_JSON_SCHEMA = "tool.json_schema"
    """
    The json schema of a tool input, It is RECOMMENDED that this be in the
    OpenAI tool calling format: https://platform.openai.com/docs/assistants/tools
    """


class Endpoints(Enum):
    FUTURE_AGI = (
        f"{get_env_collector_endpoint()}/tracer/observation-span/create_otel_span/"
    )


class FiSpanKindValues(Enum):
    TOOL = "TOOL"
    CHAIN = "CHAIN"
    LLM = "LLM"
    RETRIEVER = "RETRIEVER"
    EMBEDDING = "EMBEDDING"
    AGENT = "AGENT"
    RERANKER = "RERANKER"
    UNKNOWN = "UNKNOWN"
    GUARDRAIL = "GUARDRAIL"
    EVALUATOR = "EVALUATOR"


class FiMimeTypeValues(Enum):
    TEXT = "text/plain"
    JSON = "application/json"


class FiLLMSystemValues(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    COHERE = "cohere"
    MISTRALAI = "mistralai"
    VERTEXAI = "vertexai"


class FiLLMProviderValues(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    COHERE = "cohere"
    MISTRALAI = "mistralai"
    GOOGLE = "google"
    AZURE = "azure"
    AWS = "aws"


class ProjectType(Enum):
    EXPERIMENT = "experiment"
    OBSERVE = "observe"


class EvalTagType(Enum):
    OBSERVATION_SPAN = "OBSERVATION_SPAN_TYPE"


class EvalSpanKind(Enum):
    TOOL = "TOOL"
    LLM = "LLM"
    RETRIEVER = "RETRIEVER"
    EMBEDDING = "EMBEDDING"
    AGENT = "AGENT"
    RERANKER = "RERANKER"


class EvalName(Enum):
    CONVERSATION_COHERENCE = "Conversation Coherence"
    CONVERSATION_RESOLUTION = "Conversation Resolution"
    DETERMINISTIC_EVALS = "Deterministic Evals"
    CONTENT_MODERATION = "Content Moderation"
    CONTEXT_ADHERENCE = "Context Adherence"
    PROMPT_PERPLEXITY = "Prompt Perplexity"
    CONTEXT_RELEVANCE = "Context Relevance"
    COMPLETENESS = "Completeness"
    CONTEXT_SIMILARITY = "Context Similarity"
    PII = "PII"
    TOXICITY = "Toxicity"
    TONE = "Tone"
    SEXIST = "Sexist"
    PROMPT_INJECTION = "Prompt Injection"
    NOT_GIBBERISH_TEXT = "Not Gibberish text"
    SAFE_FOR_WORK_TEXT = "Safe for Work text"
    PROMPT_INSTRUCTION_ADHERENCE = "Prompt/Instruction Adherence"
    DATA_PRIVACY_COMPLIANCE = "Data Privacy Compliance"
    IS_JSON = "Is Json"
    ENDS_WITH = "Ends With"
    EQUALS = "Equals"
    CONTAINS_ALL = "Contains All"
    LENGTH_LESS_THAN = "Length Less Than"
    CONTAINS_NONE = "Contains None"
    REGEX = "Regex"
    STARTS_WITH = "Starts With"
    API_CALL = "API Call"
    LENGTH_BETWEEN = "Length Between"
    CUSTOM_CODE_EVALUATION = "Custom Code Evaluation"
    AGENT_AS_JUDGE = "Agent as a Judge"
    ONE_LINE = "One Line"
    CONTAINS_VALID_LINK = "Contains Valid Link"
    IS_EMAIL = "Is Email"
    LENGTH_GREATER_THAN = "Length Greater than"
    NO_VALID_LINKS = "No Valid Links"
    CONTAINS = "Contains"
    CONTAINS_ANY = "Contains Any"
    GROUNDEDNESS = "Groundedness"
    ANSWER_SIMILARITY = "Answer Similarity"
    EVAL_OUTPUT = "Eval Output"
    EVAL_CONTEXT_RETRIEVAL_QUALITY = "Eval Context Retrieval Quality"
    EVAL_IMAGE_INSTRUCTION = "Eval Image Instruction (text to image)"
    SCORE_EVAL = "Score Eval"
    SUMMARY_QUALITY = "Summary Quality"
    FACTUAL_ACCURACY = "Factual Accuracy"
    TRANSLATION_ACCURACY = "Translation Accuracy"
    CULTURAL_SENSITIVITY = "Cultural Sensitivity"
    BIAS_DETECTION = "Bias Detection"
    EVALUATE_LLM_FUNCTION_CALLING = "Evaluate LLM Function calling"
    AUDIO_TRANSCRIPTION = "Audio Transcription"
    EVAL_AUDIO_DESCRIPTION = "Eval Audio Description"
    AUDIO_QUALITY = "Audio Quality"
    JSON_SCHEMA_VALIDATION = "Json Scheme Validation"
    CHUNK_ATTRIBUTION = "Chunk Attribution"
    CHUNK_UTILIZATION = "Chunk Utilization"
    EVAL_RANKING = "Eval Ranking"


@dataclass
class ConfigField:
    type: Type
    default: Any = None
    required: bool = False


class EvalConfig:
    @staticmethod
    def get_config_for_eval(eval_name: EvalName) -> Dict[str, Dict[str, Any]]:
        configs = {
            EvalName.CONVERSATION_COHERENCE: {
                "model": ConfigField(type=str, default="gpt-4o-mini")
            },
            EvalName.CONVERSATION_RESOLUTION: {
                "model": ConfigField(type=str, default="gpt-4o-mini")
            },
            EvalName.DETERMINISTIC_EVALS: {
                "multi_choice": ConfigField(type=bool, default=False),
                "choices": ConfigField(type=list, default=[], required=True),
                "rule_prompt": ConfigField(type=str, default="", required=True),
                "input": ConfigField(type=list, default=[]),
            },
            EvalName.CONTENT_MODERATION: {},
            EvalName.CONTEXT_ADHERENCE: {
                "criteria": ConfigField(
                    type=str,
                    default="check whether output contains any information which was not provided in the context.",
                )
            },
            EvalName.PROMPT_PERPLEXITY: {
                "model": ConfigField(type=str, default="gpt-4o-mini")
            },
            EvalName.CONTEXT_RELEVANCE: {
                "check_internet": ConfigField(type=bool, default=False)
            },
            EvalName.COMPLETENESS: {},
            EvalName.CONTEXT_SIMILARITY: {
                "comparator": ConfigField(type=str, default="CosineSimilarity"),
                "failure_threshold": ConfigField(type=float, default=0.5),
            },
            EvalName.PII: {},
            EvalName.TOXICITY: {},
            EvalName.TONE: {},
            EvalName.SEXIST: {},
            EvalName.PROMPT_INJECTION: {},
            EvalName.NOT_GIBBERISH_TEXT: {},
            EvalName.SAFE_FOR_WORK_TEXT: {},
            EvalName.PROMPT_INSTRUCTION_ADHERENCE: {},
            EvalName.DATA_PRIVACY_COMPLIANCE: {
                "check_internet": ConfigField(type=bool, default=False)
            },
            EvalName.IS_JSON: {},
            EvalName.ENDS_WITH: {
                "case_sensitive": ConfigField(type=bool, default=True),
                "substring": ConfigField(type=str, default=None, required=True),
            },
            EvalName.EQUALS: {"case_sensitive": ConfigField(type=bool, default=True)},
            EvalName.CONTAINS_ALL: {
                "case_sensitive": ConfigField(type=bool, default=True),
                "keywords": ConfigField(type=list, default=[]),
            },
            EvalName.LENGTH_LESS_THAN: {
                "max_length": ConfigField(type=int, default=200)
            },
            EvalName.CONTAINS_NONE: {
                "case_sensitive": ConfigField(type=bool, default=True),
                "keywords": ConfigField(type=list, default=[]),
            },
            EvalName.REGEX: {
                "pattern": ConfigField(type=str, default=""),
            },
            EvalName.STARTS_WITH: {
                "substring": ConfigField(type=str, default=None, required=True),
                "case_sensitive": ConfigField(type=bool, default=True),
            },
            EvalName.API_CALL: {
                "url": ConfigField(type=str, default=None, required=True),
                "payload": ConfigField(type=dict, default={}),
                "headers": ConfigField(type=dict, default={}),
            },
            EvalName.LENGTH_BETWEEN: {
                "max_length": ConfigField(type=int, default=200),
                "min_length": ConfigField(type=int, default=50),
            },
            EvalName.CUSTOM_CODE_EVALUATION: {
                "code": ConfigField(type=str, default=None)
            },
            EvalName.AGENT_AS_JUDGE: {
                "model": ConfigField(type=str, default="gpt-4o-mini"),
                "eval_prompt": ConfigField(type=str, default=None, required=True),
                "system_prompt": ConfigField(type=str, default=""),
            },
            EvalName.ONE_LINE: {},
            EvalName.CONTAINS_VALID_LINK: {},
            EvalName.IS_EMAIL: {},
            EvalName.LENGTH_GREATER_THAN: {
                "min_length": ConfigField(type=int, default=50),
            },
            EvalName.NO_VALID_LINKS: {},
            EvalName.CONTAINS: {
                "case_sensitive": ConfigField(type=bool, default=True),
                "keyword": ConfigField(type=str, default=None, required=True),
            },
            EvalName.CONTAINS_ANY: {
                "case_sensitive": ConfigField(type=bool, default=True),
                "keywords": ConfigField(type=list, default=[]),
            },
            EvalName.GROUNDEDNESS: {},
            EvalName.ANSWER_SIMILARITY: {
                "comparator": ConfigField(type=str, default="CosineSimilarity"),
                "failure_threshold": ConfigField(type=float, default=0.5),
            },
            EvalName.EVAL_OUTPUT: {
                "check_internet": ConfigField(type=bool, default=False),
                "criteria": ConfigField(
                    type=str,
                    default="Check if the output follows the given input instructions, checking for completion of all requested tasks and adherence to specified constraints or formats.",
                ),
            },
            EvalName.EVAL_CONTEXT_RETRIEVAL_QUALITY: {
                "criteria": ConfigField(
                    type=str,
                    default="Evaluate if the context is relevant and sufficient to support the output.",
                )
            },
            EvalName.EVAL_IMAGE_INSTRUCTION: {
                "criteria": ConfigField(
                    type=str,
                    default="Check if the output follows the given input instructions, checking for completion of all requested tasks and adherence to specified constraints or formats.",
                )
            },
            EvalName.SCORE_EVAL: {
                "rule_prompt": ConfigField(
                    type=str,
                    default="Check if the output follows the given input instructions, checking for completion of all requested tasks and adherence to specified constraints or formats.",
                ),
                "criteria": ConfigField(type=str, default=""),
                "input": ConfigField(type=list, default=[]),
            },
            EvalName.SUMMARY_QUALITY: {
                "check_internet": ConfigField(type=bool, default=False),
                "criteria": ConfigField(
                    type=str,
                    default="Check if the summary concisely captures the main points while maintaining accuracy and relevance to the original content.",
                ),
            },
            EvalName.FACTUAL_ACCURACY: {
                "criteria": ConfigField(
                    type=str,
                    default="Check if the provided output is factually accurate based on the given information or the absence thereof.",
                ),
                "check_internet": ConfigField(type=bool, default=False),
            },
            EvalName.TRANSLATION_ACCURACY: {
                "check_internet": ConfigField(type=bool, default=False),
                "criteria": ConfigField(
                    type=str,
                    default="Check if the language translation accurately conveys the meaning and context of the input in the output.",
                ),
            },
            EvalName.CULTURAL_SENSITIVITY: {
                "criteria": ConfigField(
                    type=str,
                    default="Assesses given text for inclusivity and cultural awareness.",
                )
            },
            EvalName.BIAS_DETECTION: {
                "criteria": ConfigField(
                    type=str,
                    default="check whether given text has any forms of bias, promoting unfairness and unneutrality in it. Looking that input and context if provided.. If it is biased then return Failed else return Passed",
                )
            },
            EvalName.EVALUATE_LLM_FUNCTION_CALLING: {
                "criteria": ConfigField(
                    type=str,
                    default="Assess whether the output correctly identifies the need for a tool call and accurately includes the tool with the appropriate parameters extracted from the input.",
                )
            },
            EvalName.AUDIO_TRANSCRIPTION: {
                "criteria": ConfigField(
                    type=str,
                    default="determine the accuracy of the transcription of the given audio",
                )
            },
            EvalName.EVAL_AUDIO_DESCRIPTION: {
                "criteria": ConfigField(
                    type=str,
                    default="determine the if the description of the given audio matches the given audio",
                ),
                "model": ConfigField(type=str, default="gemini-2.0-flash"),
            },
            EvalName.AUDIO_QUALITY: {
                "criteria": ConfigField(
                    type=str,
                    default="determine the quality of the given audio",
                ),
                "model": ConfigField(type=str, default="gemini-2.0-flash"),
            },
            EvalName.JSON_SCHEMA_VALIDATION: {
                "validations": ConfigField(type=list, default=[]),
            },
            EvalName.CHUNK_ATTRIBUTION: {},
            EvalName.CHUNK_UTILIZATION: {},
            EvalName.EVAL_RANKING: {
                "criteria": ConfigField(
                    type=str,
                    default="Check if the summary concisely captures the main points while maintaining accuracy and relevance to the original content."
                ),
            },
        }

        # Convert ConfigField objects to dictionary format
        if eval_name in configs:
            return {
                key: {
                    "type": field.type,
                    "default": field.default,
                    "required": field.required,
                }
                for key, field in configs[eval_name].items()
            }

        else:
            raise ValueError(f"No eval found with the following name: {eval_name}")


class EvalMappingConfig:
    @staticmethod
    def get_mapping_for_eval(eval_name: EvalName) -> Dict[str, Dict[str, Any]]:
        mappings = {
            EvalName.CONVERSATION_COHERENCE: {
                "output": ConfigField(type=str, required=True)
            },
            EvalName.CONVERSATION_RESOLUTION: {
                "output": ConfigField(type=str, required=True)
            },
            EvalName.DETERMINISTIC_EVALS: {},
            EvalName.CONTENT_MODERATION: {"text": ConfigField(type=str, required=True)},
            EvalName.CONTEXT_ADHERENCE: {
                "context": ConfigField(type=str, required=True),
                "output": ConfigField(type=str, required=True),
            },
            EvalName.PROMPT_PERPLEXITY: {"input": ConfigField(type=str, required=True)},
            EvalName.CONTEXT_RELEVANCE: {
                "context": ConfigField(type=str, required=True),
                "input": ConfigField(type=str, required=True),
            },
            EvalName.COMPLETENESS: {
                "input": ConfigField(type=str, required=True),
                "output": ConfigField(type=str, required=True),
            },
            EvalName.CONTEXT_SIMILARITY: {
                "context": ConfigField(type=str, required=True),
                "response": ConfigField(type=str, required=True),
            },
            EvalName.PII: {"input": ConfigField(type=str, required=True)},
            EvalName.TOXICITY: {"input": ConfigField(type=str, required=True)},
            EvalName.TONE: {"input": ConfigField(type=str, required=True)},
            EvalName.SEXIST: {"input": ConfigField(type=str, required=True)},
            EvalName.PROMPT_INJECTION: {"input": ConfigField(type=str, required=True)},
            EvalName.SAFE_FOR_WORK_TEXT: {
                "response": ConfigField(type=str, required=True)
            },
            EvalName.NOT_GIBBERISH_TEXT: {
                "response": ConfigField(type=str, required=True)
            },
            EvalName.PROMPT_INSTRUCTION_ADHERENCE: {
                "output": ConfigField(type=str, required=True)
            },
            EvalName.DATA_PRIVACY_COMPLIANCE: {
                "input": ConfigField(type=str, required=True)
            },
            EvalName.IS_JSON: {"text": ConfigField(type=str, required=True)},
            EvalName.ENDS_WITH: {"text": ConfigField(type=str, required=True)},
            EvalName.EQUALS: {
                "text": ConfigField(type=str, required=True),
                "expected_text": ConfigField(
                    type=str, default="expected", required=True
                ),
            },
            EvalName.CONTAINS_ALL: {"text": ConfigField(type=str, required=True)},
            EvalName.LENGTH_LESS_THAN: {"text": ConfigField(type=str, required=True)},
            EvalName.CONTAINS_NONE: {"text": ConfigField(type=str, required=True)},
            EvalName.REGEX: {"text": ConfigField(type=str, required=True)},
            EvalName.STARTS_WITH: {"text": ConfigField(type=str, required=True)},
            EvalName.LENGTH_BETWEEN: {"text": ConfigField(type=str, required=True)},
            EvalName.ONE_LINE: {"text": ConfigField(type=str, required=True)},
            EvalName.CONTAINS_VALID_LINK: {
                "text": ConfigField(type=str, required=True)
            },
            EvalName.IS_EMAIL: {"text": ConfigField(type=str, required=True)},
            EvalName.LENGTH_GREATER_THAN: {
                "text": ConfigField(type=str, required=True)
            },
            EvalName.NO_VALID_LINKS: {"text": ConfigField(type=str, required=True)},
            EvalName.CONTAINS: {"text": ConfigField(type=str, required=True)},
            EvalName.CONTAINS_ANY: {"text": ConfigField(type=str, required=True)},
            EvalName.GROUNDEDNESS: {
                "output": ConfigField(type=str, required=True),
                "input": ConfigField(type=str, required=True),
            },
            EvalName.ANSWER_SIMILARITY: {
                "response": ConfigField(type=str, required=True),
                "expected_response": ConfigField(type=str, required=True),
            },
            EvalName.EVAL_OUTPUT: {
                "input": ConfigField(type=str),
                "output": ConfigField(type=str, required=True),
                "context": ConfigField(type=str),
            },
            EvalName.EVAL_CONTEXT_RETRIEVAL_QUALITY: {
                "context": ConfigField(type=str),
                "input": ConfigField(type=str),
                "output": ConfigField(type=str),
            },
            EvalName.EVAL_IMAGE_INSTRUCTION: {
                "input": ConfigField(type=str, required=True),
                "image_url": ConfigField(type=str, required=True),
            },
            EvalName.SCORE_EVAL: {},
            EvalName.SUMMARY_QUALITY: {
                "input": ConfigField(type=str),
                "output": ConfigField(type=str, required=True),
                "context": ConfigField(type=str),
            },
            EvalName.FACTUAL_ACCURACY: {
                "input": ConfigField(type=str),
                "output": ConfigField(type=str, required=True),
                "context": ConfigField(type=str),
            },
            EvalName.TRANSLATION_ACCURACY: {
                "input": ConfigField(type=str, required=True),
                "output": ConfigField(type=str, required=True),
            },
            EvalName.CULTURAL_SENSITIVITY: {
                "input": ConfigField(type=str, required=True)
            },
            EvalName.BIAS_DETECTION: {"input": ConfigField(type=str, required=True)},
            EvalName.EVALUATE_LLM_FUNCTION_CALLING: {
                "input": ConfigField(type=str, required=True),
                "output": ConfigField(type=str, required=True),
            },
            EvalName.API_CALL: {"response": ConfigField(type=str, required=True)},
            EvalName.CUSTOM_CODE_EVALUATION: {},
            EvalName.AGENT_AS_JUDGE: {},
            EvalName.AUDIO_TRANSCRIPTION: {
                "input audio": ConfigField(type=str, required=True),
                "input transcription": ConfigField(type=str, required=True),
            },
            EvalName.EVAL_AUDIO_DESCRIPTION: {
                "input audio": ConfigField(type=str, required=True)
            },
            EvalName.AUDIO_QUALITY: {
                "input audio": ConfigField(type=str, required=True)
            },
            EvalName.JSON_SCHEMA_VALIDATION: {
                "actual_json": ConfigField(type=dict, required=True),
                "expected_json": ConfigField(type=str, required=True),
            },
            EvalName.CHUNK_ATTRIBUTION: {
                "input": ConfigField(type=str),
                "output": ConfigField(type=str, required=True),
                "context": ConfigField(type=str, required=True),
            },
            EvalName.CHUNK_UTILIZATION: {
                "input": ConfigField(type=str),
                "output": ConfigField(type=str, required=True),
                "context": ConfigField(type=str, required=True),
            },
            EvalName.EVAL_RANKING: {
                "input": ConfigField(type=str, required=True),
                "context" : ConfigField(type=str, required=True),
            },
        }

        # Convert ConfigField objects to dictionary format
        if eval_name in mappings:
            return {
                key: {
                    "type": field.type,
                    "default": field.default,
                    "required": field.required,
                }
                for key, field in mappings[eval_name].items()
            }
        else:
            raise ValueError(f"No mapping definition found for eval: {eval_name}")


@dataclass
class EvalTag:
    type: EvalTagType
    value: EvalSpanKind
    eval_name: EvalName
    config: Dict[str, Any] = None
    custom_eval_name: str = ""
    mapping: Dict[str, str] = None

    def __post_init__(self):
        if self.config is None:
            self.config = {}
        if self.mapping is None:
            self.mapping = {}
            
        if not isinstance(self.value, EvalSpanKind):
            raise ValueError(
                f"value must be a EvalSpanKind enum, got {type(self.value)}"
            )

        if not isinstance(self.type, EvalTagType):
            raise ValueError(f"type must be an EvalTagType enum, got {type(self.type)}")

        if not isinstance(self.eval_name, EvalName):
            raise ValueError(
                f"eval_name must be an EvalName enum, got {type(self.eval_name)}"
            )

        if not isinstance(self.config, dict):
            raise ValueError(f"config must be a dictionary, got {type(self.config)}")

        expected_config = EvalConfig.get_config_for_eval(self.eval_name)

        for key, field_config in expected_config.items():
            if key not in self.config:
                if field_config["required"]:
                    raise ValueError(
                        f"Required field '{key}' is missing from config for {self.eval_name.value}"
                    )
                self.config[key] = field_config["default"]
            else:
                self._validate_field_type(key, field_config["type"], self.config[key])

        for key in self.config:
            if key not in expected_config:
                raise ValueError(
                    f"Unexpected field '{key}' in config for {self.eval_name.value}. Allowed fields are: {list(expected_config.keys())}"
                )

        expected_mapping = EvalMappingConfig.get_mapping_for_eval(self.eval_name)

        if not isinstance(self.mapping, dict):
            raise ValueError(f"mapping must be a dictionary, got {type(self.mapping)}")

        for key, field_config in expected_mapping.items():
            if field_config["required"] and key not in self.mapping:
                raise ValueError(
                    f"Required mapping field '{key}' is missing for {self.eval_name.value}"
                )

        for key, value in self.mapping.items():
            if key not in expected_mapping:
                raise ValueError(
                    f"Unexpected mapping field '{key}' for {self.eval_name.value}. Allowed fields are: {list(expected_mapping.keys())}"
                )
            if not isinstance(key, str):
                raise ValueError(f"All mapping keys must be strings, got {type(key)}")
            if not isinstance(value, str):
                raise ValueError(
                    f"All mapping values must be strings, got {type(value)}"
                )

    def _validate_field_type(self, key: str, expected_type: Type, value: Any) -> None:
        """Validate field type according to configuration"""

        if not isinstance(value, expected_type):
            raise ValueError(f"Field '{key}' must be of type '{expected_type.__name__}', got '{type(value).__name__}' instead.")



    def to_dict(self) -> Dict[str, Any]:
        """Convert EvalTag to dictionary format for API responses"""
        return {
            "type": self.type.value,
            "value": self.value.value,
            "eval_name": self.eval_name.value,
            "config": self.config,
            "mapping": self.mapping,
            "custom_eval_name": self.custom_eval_name,
        }

    def __str__(self) -> str:
        """String representation for debugging"""
        return f"EvalTag(type={self.type.value}, value={self.value.value}, eval_name={self.eval_name.value})"


def prepare_eval_tags(eval_tags: List[EvalTag]) -> List[Dict[str, Any]]:
    """Convert list of EvalTag objects to list of dictionaries for API consumption"""
    return [tag.to_dict() for tag in eval_tags]
