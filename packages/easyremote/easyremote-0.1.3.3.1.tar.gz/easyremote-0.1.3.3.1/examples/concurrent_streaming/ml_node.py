#!/usr/bin/env python3
"""
æœºå™¨å­¦ä¹ è®¡ç®—èŠ‚ç‚¹
å®ç°å›¾åƒåˆ†ç±»çš„æµå¼æ¨ç†ä»»åŠ¡
"""
import asyncio
import time
import random
import logging
from typing import Dict, Generator
from datetime import datetime
from easyremote import ComputeNode

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MockImageClassifier:
    """æ¨¡æ‹Ÿå›¾åƒåˆ†ç±»å™¨"""
    
    def __init__(self):
        self.model_info = {
            'mobilenet_v2': {
                'classes': 1000,
                'input_size': (224, 224),
                'inference_time': 0.05  # seconds
            },
            'resnet50': {
                'classes': 1000,
                'input_size': (224, 224),
                'inference_time': 0.08
            },
            'efficientnet_b0': {
                'classes': 1000,
                'input_size': (224, 224),
                'inference_time': 0.06
            }
        }
        
        # ImageNet ç±»åˆ«ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.imagenet_classes = [
            'dog', 'cat', 'bird', 'car', 'airplane',
            'ship', 'truck', 'horse', 'elephant', 'bear',
            'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',
            'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',
            'sports_ball', 'kite', 'baseball_bat', 'keyboard', 'mouse',
            'remote', 'microwave', 'oven', 'toaster', 'sink',
            'refrigerator', 'book', 'clock', 'vase', 'scissors'
        ]
        
        self.inference_count = 0
        
    def generate_mock_image(self) -> Dict:
        """ç”Ÿæˆæ¨¡æ‹Ÿå›¾åƒæ•°æ®"""
        # æ¨¡æ‹Ÿç”Ÿæˆå›¾åƒå…ƒæ•°æ®
        width, height = random.choice([(224, 224), (256, 256), (299, 299)])
        
        return {
            'width': width,
            'height': height,
            'channels': 3,
            'format': 'RGB',
            'size_bytes': width * height * 3,
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæ˜¯çœŸå®çš„å›¾åƒæ•°æ®
            'data': f"mock_image_{self.inference_count}.jpg"
        }
    
    def classify_image(self, image_data: Dict, model_name: str) -> Dict:
        """å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»"""
        if model_name not in self.model_info:
            model_name = 'mobilenet_v2'
            
        model_config = self.model_info[model_name]
        
        # æ¨¡æ‹Ÿæ¨ç†æ—¶é—´
        time.sleep(model_config['inference_time'])
        
        # ç”Ÿæˆéšæœºé¢„æµ‹ç»“æœ
        num_predictions = min(5, len(self.imagenet_classes))
        predicted_classes = random.sample(self.imagenet_classes, num_predictions)
        
        # ç”Ÿæˆç½®ä¿¡åº¦åˆ†æ•°ï¼ˆç¡®ä¿æ€»å’Œæ¥è¿‘1.0ï¼‰
        raw_scores = [random.random() for _ in range(num_predictions)]
        total = sum(raw_scores)
        confidences = [score/total for score in raw_scores]
        confidences.sort(reverse=True)
        
        predictions = []
        for i, class_name in enumerate(predicted_classes[:num_predictions]):
            predictions.append({
                'class': class_name,
                'confidence': round(confidences[i], 4),
                'class_id': random.randint(0, 999)
            })
        
        self.inference_count += 1
        
        return {
            'predictions': predictions,
            'top_class': predictions[0]['class'],
            'top_confidence': predictions[0]['confidence'],
            'model_used': model_name,
            'inference_time': model_config['inference_time'],
            'preprocessing_time': random.uniform(0.001, 0.005)
        }

# åˆ›å»ºè®¡ç®—èŠ‚ç‚¹
node = ComputeNode("localhost:8080", node_id="ml-node")
classifier = MockImageClassifier()

@node.register(stream=True, name="classify_image_stream")
def classify_image_stream(model_config: dict) -> Generator[Dict, None, None]:
    """
    å›¾åƒåˆ†ç±»æµå¼æ¨ç†
    
    Args:
        model_config: æ¨¡å‹é…ç½®
            - model_name: æ¨¡å‹åç§°
            - batch_size: æ‰¹å¤„ç†å¤§å°
            - num_images: å¤„ç†å›¾åƒæ•°é‡
            - delay: å›¾åƒé—´å»¶è¿Ÿ (optional)
    
    Yields:
        Dict: åˆ†ç±»ç»“æœ
    """
    model_name = model_config.get('model_name', 'mobilenet_v2')
    batch_size = model_config.get('batch_size', 1)
    num_images = model_config.get('num_images', 10)
    delay = model_config.get('delay', 0.1)  # å›¾åƒé—´å»¶è¿Ÿ
    
    logger.info(f"ğŸ¤– Starting image classification stream")
    logger.info(f"ğŸ“ Model: {model_name}, Batch size: {batch_size}, Images: {num_images}")
    
    start_time = time.time()
    processed_images = 0
    batch_images = []
    
    try:
        for image_idx in range(num_images):
            # ç”Ÿæˆæ¨¡æ‹Ÿå›¾åƒ
            image_data = classifier.generate_mock_image()
            batch_images.append({
                'image_id': f"img_{image_idx:04d}",
                'data': image_data
            })
            
            # å½“æ‰¹æ¬¡æ»¡äº†æˆ–è€…æ˜¯æœ€åä¸€æ‰¹æ—¶ï¼Œå¤„ç†æ‰¹æ¬¡
            if len(batch_images) >= batch_size or image_idx == num_images - 1:
                batch_start_time = time.time()
                batch_results = []
                
                for img_item in batch_images:
                    # å¯¹æ¯å¼ å›¾åƒè¿›è¡Œåˆ†ç±»
                    classification_result = classifier.classify_image(
                        img_item['data'], model_name
                    )
                    
                    batch_results.append({
                        'image_id': img_item['image_id'],
                        'image_info': img_item['data'],
                        'classification': classification_result,
                        'timestamp': datetime.now().isoformat()
                    })
                
                batch_time = time.time() - batch_start_time
                processed_images += len(batch_images)
                
                # æ„é€ æ‰¹æ¬¡ç»“æœ
                result = {
                    'batch_id': f"batch_{(image_idx // batch_size) + 1:03d}",
                    'batch_size': len(batch_images),
                    'batch_time': round(batch_time, 4),
                    'throughput': round(len(batch_images) / batch_time, 2),  # images/second
                    'total_processed': processed_images,
                    'progress': round((processed_images / num_images) * 100, 1),
                    'results': batch_results,
                    'model_info': {
                        'name': model_name,
                        'total_inferences': classifier.inference_count
                    },
                    'node_id': 'ml-node',
                    'status': 'processing'
                }
                
                # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                elapsed_time = time.time() - start_time
                if elapsed_time > 0:
                    result['overall_throughput'] = round(processed_images / elapsed_time, 2)
                    result['estimated_completion'] = round(
                        (num_images - processed_images) / (processed_images / elapsed_time), 1
                    ) if processed_images > 0 else 0
                
                yield result
                
                # æ¸…ç©ºæ‰¹æ¬¡ç¼“å†²åŒº
                batch_images = []
                
                # æ·»åŠ å»¶è¿Ÿï¼ˆå¦‚æœéœ€è¦ï¼‰
                if delay > 0:
                    time.sleep(delay)
        
        # å‘é€å®Œæˆä¿¡å·
        final_result = {
            'batch_id': 'final',
            'status': 'completed',
            'total_processed': processed_images,
            'total_time': round(time.time() - start_time, 2),
            'overall_throughput': round(processed_images / (time.time() - start_time), 2),
            'model_info': {
                'name': model_name,
                'total_inferences': classifier.inference_count
            },
            'node_id': 'ml-node'
        }
        
        yield final_result
        
    except Exception as e:
        logger.error(f"âŒ Error in image classification stream: {e}")
        yield {
            'batch_id': 'error',
            'status': 'error',
            'error': str(e),
            'total_processed': processed_images,
            'node_id': 'ml-node',
            'timestamp': datetime.now().isoformat()
        }
    
    logger.info(f"âœ… Image classification stream completed: {processed_images} images")

@node.register(name="get_model_info")
def get_model_info() -> Dict:
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    return {
        'node_id': 'ml-node',
        'status': 'online',
        'available_models': list(classifier.model_info.keys()),
        'model_details': classifier.model_info,
        'supported_formats': ['RGB', 'BGR', 'RGBA'],
        'capabilities': {
            'max_batch_size': 32,
            'gpu_acceleration': False,  # æ¨¡æ‹Ÿç¯å¢ƒ
            'concurrent_streams': 4,
            'supported_image_sizes': ['224x224', '256x256', '299x299']
        },
        'statistics': {
            'total_inferences': classifier.inference_count,
            'uptime': time.time(),  # ç®€åŒ–çš„è¿è¡Œæ—¶é—´
        }
    }

@node.register(name="benchmark_model")
def benchmark_model(model_name: str, num_samples: int = 10) -> Dict:
    """å¯¹æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
    if model_name not in classifier.model_info:
        return {'error': f'Model {model_name} not found'}
    
    logger.info(f"ğŸ§ª Benchmarking model: {model_name}")
    
    start_time = time.time()
    inference_times = []
    
    for i in range(num_samples):
        image_data = classifier.generate_mock_image()
        inference_start = time.time()
        classifier.classify_image(image_data, model_name)
        inference_time = time.time() - inference_start
        inference_times.append(inference_time)
    
    total_time = time.time() - start_time
    
    return {
        'model_name': model_name,
        'num_samples': num_samples,
        'total_time': round(total_time, 4),
        'avg_inference_time': round(sum(inference_times) / len(inference_times), 4),
        'min_inference_time': round(min(inference_times), 4),
        'max_inference_time': round(max(inference_times), 4),
        'throughput': round(num_samples / total_time, 2),
        'node_id': 'ml-node'
    }

async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ Starting ML Inference Node...")
    logger.info("ğŸ¤– Registered functions:")
    logger.info("  - classify_image_stream (streaming)")
    logger.info("  - get_model_info")
    logger.info("  - benchmark_model")
    logger.info(f"ğŸ“Š Available models: {list(classifier.model_info.keys())}")
    
    try:
        node.serve(blocking=True)
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  Received shutdown signal")
        node.stop()
        logger.info("âœ… ML node stopped gracefully")

if __name__ == "__main__":
    asyncio.run(main()) 