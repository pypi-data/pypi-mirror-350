#!/usr/bin/env python3
"""
å¹¶å‘æµå¼ä»»åŠ¡å®¢æˆ·ç«¯æ¼”ç¤º
åŒæ—¶è¿è¡Œæ•°æ®å¤„ç†æµå’Œæœºå™¨å­¦ä¹ æ¨ç†æµ
"""
import asyncio
import time
from datetime import datetime
from typing import Dict, Any
import logging
from rich.console import Console
from rich.table import Table
from rich.live import Live
from rich.panel import Panel

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºRichæ§åˆ¶å°
console = Console()

class ConcurrentStreamDemo:
    """å¹¶å‘æµå¼ä»»åŠ¡æ¼”ç¤º"""
    
    def __init__(self, vps_address: str = "localhost:8080"):
        self.vps_address = vps_address
        self.data_stream_results = []
        self.ml_stream_results = []
        self.data_stream_active = False
        self.ml_stream_active = False
        self.start_time = None
        
    def test_node_connections(self) -> bool:
        """æµ‹è¯•èŠ‚ç‚¹è¿æ¥"""
        console.print("ğŸ”— Testing node connections...", style="blue")
        
        # æ¼”ç¤ºæ¨¡å¼ï¼šæ¨¡æ‹Ÿè¿æ¥æµ‹è¯•
        console.print("âš ï¸  Running in demo mode (nodes not available)", style="yellow")
        return self._demo_mode_connections()
    
    def _demo_mode_connections(self) -> bool:
        """æ¼”ç¤ºæ¨¡å¼çš„è¿æ¥æµ‹è¯•"""
        console.print("âœ… Data node connected (demo): data-node", style="green")
        console.print("âœ… ML node connected (demo): ml-node", style="green")
        console.print("ğŸ“Š Available models: mobilenet_v2, resnet50, efficientnet_b0", style="cyan")
        return True
    
    async def run_data_stream(self, config: Dict[str, Any]):
        """è¿è¡Œæ•°æ®å¤„ç†æµ"""
        console.print("ğŸ“Š Starting data processing stream...", style="blue")
        self.data_stream_active = True
        
        try:
            # æ¼”ç¤ºæ¨¡å¼ï¼šç›´æ¥ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            await self._demo_data_stream(config)
            
        except Exception as e:
            console.print(f"âŒ Data stream error: {e}", style="red")
        finally:
            self.data_stream_active = False
            console.print("âœ… Data stream completed", style="green")
    
    async def run_ml_stream(self, config: Dict[str, Any]):
        """è¿è¡ŒMLæ¨ç†æµ"""
        console.print("ğŸ¤– Starting ML inference stream...", style="blue")
        self.ml_stream_active = True
        
        try:
            # æ¼”ç¤ºæ¨¡å¼ï¼šç›´æ¥ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            await self._demo_ml_stream(config)
            
        except Exception as e:
            console.print(f"âŒ ML stream error: {e}", style="red")
        finally:
            self.ml_stream_active = False
            console.print("âœ… ML stream completed", style="green")
    
    async def _demo_data_stream(self, config: Dict[str, Any]):
        """æ¼”ç¤ºæ¨¡å¼çš„æ•°æ®æµ"""
        import random
        
        duration = config.get('duration', 20)
        sample_rate = config.get('sample_rate', 2)
        sensors = config.get('sensors', ['temperature', 'humidity'])
        
        samples = int(duration * sample_rate)
        
        for i in range(samples):
            # ç”Ÿæˆæ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®
            readings = {}
            statistics = {}
            
            for sensor in sensors:
                base_values = {'temperature': 25.0, 'humidity': 60.0, 'pressure': 1013.25}
                value = base_values.get(sensor, 0) + random.gauss(0, 1)
                readings[sensor] = round(value, 2)
                
                statistics[sensor] = {
                    'mean': round(value + random.gauss(0, 0.1), 2),
                    'std': round(abs(random.gauss(0.5, 0.2)), 2),
                    'trend': random.choice(['increasing', 'decreasing', 'stable'])
                }
            
            result = {
                'timestamp': datetime.now().isoformat(),
                'sample_id': i + 1,
                'elapsed_time': round(i / sample_rate, 2),
                'readings': readings,
                'statistics': statistics,
                'health_status': 'healthy',
                'node_id': 'data-node'
            }
            
            self.data_stream_results.append(result)
            await asyncio.sleep(1.0 / sample_rate)
    
    async def _demo_ml_stream(self, config: Dict[str, Any]):
        """æ¼”ç¤ºæ¨¡å¼çš„MLæ¨ç†æµ"""
        import random
        
        model_name = config.get('model_name', 'mobilenet_v2')
        batch_size = config.get('batch_size', 4)
        num_images = config.get('num_images', 20)
        
        classes = ['dog', 'cat', 'bird', 'car', 'airplane', 'ship', 'truck', 'horse']
        
        num_batches = (num_images + batch_size - 1) // batch_size
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, num_images)
            current_batch_size = end_idx - start_idx
            
            # æ¨¡æ‹Ÿæ‰¹å¤„ç†æ—¶é—´
            batch_time = current_batch_size * 0.1  # æ¯å¼ å›¾ç‰‡0.1ç§’
            
            batch_results = []
            for img_idx in range(start_idx, end_idx):
                classification = {
                    'predictions': [
                        {
                            'class': random.choice(classes),
                            'confidence': round(random.uniform(0.7, 0.95), 4),
                            'class_id': random.randint(0, 999)
                        }
                    ],
                    'model_used': model_name,
                    'inference_time': 0.05
                }
                
                batch_results.append({
                    'image_id': f"img_{img_idx:04d}",
                    'classification': classification,
                    'timestamp': datetime.now().isoformat()
                })
            
            result = {
                'batch_id': f"batch_{batch_idx + 1:03d}",
                'batch_size': current_batch_size,
                'batch_time': round(batch_time, 4),
                'throughput': round(current_batch_size / batch_time, 2),
                'total_processed': end_idx,
                'progress': round((end_idx / num_images) * 100, 1),
                'results': batch_results,
                'model_info': {'name': model_name},
                'node_id': 'ml-node',
                'status': 'processing' if end_idx < num_images else 'completed'
            }
            
            self.ml_stream_results.append(result)
            await asyncio.sleep(batch_time)
    
    def create_status_table(self) -> Table:
        """åˆ›å»ºçŠ¶æ€è¡¨æ ¼"""
        table = Table(show_header=True, header_style="bold magenta")
        table.add_column("Stream", style="cyan")
        table.add_column("Status", style="green")
        table.add_column("Progress", style="yellow")
        table.add_column("Latest Result", style="white")
        
        # æ•°æ®æµçŠ¶æ€
        data_status = "ğŸŸ¢ Active" if self.data_stream_active else "ğŸ”´ Inactive"
        data_progress = f"{len(self.data_stream_results)} samples"
        data_latest = ""
        if self.data_stream_results:
            latest = self.data_stream_results[-1]
            temp = latest.get('readings', {}).get('temperature', 'N/A')
            data_latest = f"Temp: {temp}Â°C"
        
        table.add_row("Data Processing", data_status, data_progress, data_latest)
        
        # MLæµçŠ¶æ€
        ml_status = "ğŸŸ¢ Active" if self.ml_stream_active else "ğŸ”´ Inactive"
        ml_progress = f"{len(self.ml_stream_results)} batches"
        ml_latest = ""
        if self.ml_stream_results:
            latest = self.ml_stream_results[-1]
            progress = latest.get('progress', 0)
            ml_latest = f"Progress: {progress}%"
        
        table.add_row("ML Inference", ml_status, ml_progress, ml_latest)
        
        return table
    
    def create_summary_panel(self) -> Panel:
        """åˆ›å»ºæ‘˜è¦é¢æ¿"""
        if not self.start_time:
            return Panel("Starting...", title="Summary")
        
        elapsed = time.time() - self.start_time
        data_rate = len(self.data_stream_results) / elapsed if elapsed > 0 else 0
        ml_rate = len(self.ml_stream_results) / elapsed if elapsed > 0 else 0
        
        summary_text = f"""
ğŸ“Š Data Stream: {len(self.data_stream_results)} samples ({data_rate:.1f}/s)
ğŸ¤– ML Stream: {len(self.ml_stream_results)} batches ({ml_rate:.1f}/s)
â±ï¸  Elapsed Time: {elapsed:.1f}s
ğŸ”„ Both streams active: {self.data_stream_active and self.ml_stream_active}
        """.strip()
        
        return Panel(summary_text, title="Real-time Summary", border_style="blue")
    
    async def run_concurrent_demo(self):
        """è¿è¡Œå¹¶å‘æ¼”ç¤º"""
        console.print("\nğŸš€ Starting Concurrent Streaming Demo", style="bold blue")
        console.print("=" * 50)
        
        # æµ‹è¯•è¿æ¥
        if not self.test_node_connections():
            console.print("âŒ Connection test failed. Exiting.", style="red")
            return
        
        console.print("\nâš¡ Configuring streaming tasks...", style="blue")
        
        # é…ç½®æ•°æ®å¤„ç†æµ
        data_config = {
            'sensors': ['temperature', 'humidity', 'pressure'],
            'sample_rate': 2,  # 2 samples/second
            'duration': 20     # 20 seconds
        }
        
        # é…ç½®MLæ¨ç†æµ
        ml_config = {
            'model_name': 'mobilenet_v2',
            'batch_size': 4,
            'num_images': 24,
            'delay': 0.5
        }
        
        console.print(f"ğŸ“Š Data stream: {data_config['sensors']} @ {data_config['sample_rate']} Hz", style="cyan")
        console.print(f"ğŸ¤– ML stream: {ml_config['model_name']} ({ml_config['num_images']} images)", style="cyan")
        
        self.start_time = time.time()
        
        # åˆ›å»ºå®æ—¶æ˜¾ç¤º
        with Live(self.create_status_table(), refresh_per_second=2) as live:
            # å¯åŠ¨å¹¶å‘ä»»åŠ¡
            tasks = [
                asyncio.create_task(self.run_data_stream(data_config)),
                asyncio.create_task(self.run_ml_stream(ml_config))
            ]
            
            # ç›‘æ§ä»»åŠ¡è¿›åº¦
            while any(not task.done() for task in tasks):
                # æ›´æ–°æ˜¾ç¤º
                live.update(self.create_status_table())
                await asyncio.sleep(0.5)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            await asyncio.gather(*tasks)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        console.print("\n" + "=" * 50)
        console.print("ğŸ‰ Demo Completed!", style="bold green")
        console.print(self.create_summary_panel())
        
        # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
        self._show_detailed_stats()
    
    def _show_detailed_stats(self):
        """æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        console.print("\nğŸ“ˆ Detailed Statistics", style="bold blue")
        
        # æ•°æ®æµç»Ÿè®¡
        if self.data_stream_results:
            console.print("\nğŸ“Š Data Processing Stream:", style="cyan")
            total_samples = len(self.data_stream_results)
            if total_samples > 0:
                latest_result = self.data_stream_results[-1]
                total_time = latest_result.get('elapsed_time', 0)
                console.print(f"  â€¢ Total samples: {total_samples}")
                console.print(f"  â€¢ Duration: {total_time:.1f}s")
                console.print(f"  â€¢ Average rate: {total_samples/total_time:.2f} samples/s" if total_time > 0 else "  â€¢ Rate: N/A")
                
                # æ˜¾ç¤ºæœ€æ–°è¯»æ•°
                if 'readings' in latest_result:
                    console.print("  â€¢ Latest readings:")
                    for sensor, value in latest_result['readings'].items():
                        console.print(f"    - {sensor}: {value}")
        
        # MLæµç»Ÿè®¡
        if self.ml_stream_results:
            console.print("\nğŸ¤– ML Inference Stream:", style="cyan")
            total_batches = len(self.ml_stream_results)
            total_images = 0
            total_inference_time = 0
            
            for result in self.ml_stream_results:
                if result.get('status') != 'error':
                    total_images += result.get('batch_size', 0)
                    total_inference_time += result.get('batch_time', 0)
            
            console.print(f"  â€¢ Total batches: {total_batches}")
            console.print(f"  â€¢ Total images: {total_images}")
            console.print(f"  â€¢ Total inference time: {total_inference_time:.2f}s")
            
            if total_inference_time > 0:
                console.print(f"  â€¢ Average throughput: {total_images/total_inference_time:.2f} images/s")
            
            # æ˜¾ç¤ºæœ€æ–°åˆ†ç±»ç»“æœ
            if self.ml_stream_results and 'results' in self.ml_stream_results[-1]:
                latest_batch = self.ml_stream_results[-1]['results']
                if latest_batch:
                    latest_classification = latest_batch[-1]['classification']
                    top_pred = latest_classification['predictions'][0]
                    console.print(f"  â€¢ Latest classification: {top_pred['class']} ({top_pred['confidence']:.3f})")

async def main():
    """ä¸»å‡½æ•°"""
    console.print("ğŸ¯ EasyRemote Concurrent Streaming Demo", style="bold magenta")
    console.print("Demonstrating parallel data processing and ML inference streams\n")
    
    demo = ConcurrentStreamDemo()
    
    try:
        await demo.run_concurrent_demo()
    except KeyboardInterrupt:
        console.print("\nâš ï¸  Demo interrupted by user", style="yellow")
    except Exception as e:
        console.print(f"\nâŒ Demo failed: {e}", style="red")
        raise

if __name__ == "__main__":
    asyncio.run(main()) 