models:
  # OpenAI models
  gpt-3.5-turbo:
    provider: openai
    tokenizer: gpt
    description: "GPT-3.5 Turbo model by OpenAI"
    input_cost_per_1k: 0.0015
    output_cost_per_1k: 0.002
    max_tokens: 16385
    context_window: 16385

  gpt-3.5-turbo-16k:
    provider: openai
    tokenizer: gpt
    description: "GPT-3.5 Turbo 16K by OpenAI"
    input_cost_per_1k: 0.003
    output_cost_per_1k: 0.004
    max_tokens: 16385
    context_window: 16385

  gpt-4:
    provider: openai
    tokenizer: gpt
    description: "GPT-4 model by OpenAI"
    input_cost_per_1k: 0.03
    output_cost_per_1k: 0.06
    max_tokens: 8192
    context_window: 8192

  gpt-4-32k:
    provider: openai
    tokenizer: gpt
    description: "GPT-4 32K model by OpenAI"
    input_cost_per_1k: 0.06
    output_cost_per_1k: 0.12
    max_tokens: 32768
    context_window: 32768

  gpt-4-turbo:
    provider: openai
    tokenizer: gpt
    description: "GPT-4 Turbo model by OpenAI"
    input_cost_per_1k: 0.01
    output_cost_per_1k: 0.03
    max_tokens: 128000
    context_window: 128000

  gpt-4o:
    provider: openai
    tokenizer: gpt
    description: "GPT-4o model by OpenAI"
    input_cost_per_1k: 0.005
    output_cost_per_1k: 0.015
    max_tokens: 128000
    context_window: 128000

  # Anthropic models
  claude-instant-1:
    provider: anthropic
    tokenizer: claude
    description: "Claude Instant 1 by Anthropic"
    input_cost_per_1k: 0.0008
    output_cost_per_1k: 0.0024
    max_tokens: 100000
    context_window: 100000

  claude-2:
    provider: anthropic
    tokenizer: claude
    description: "Claude 2 by Anthropic"
    input_cost_per_1k: 0.0080
    output_cost_per_1k: 0.0240
    max_tokens: 100000
    context_window: 100000

  claude-3-opus:
    provider: anthropic
    tokenizer: claude
    description: "Claude 3 Opus by Anthropic"
    input_cost_per_1k: 0.015
    output_cost_per_1k: 0.075
    max_tokens: 200000
    context_window: 200000

  claude-3-sonnet:
    provider: anthropic
    tokenizer: claude
    description: "Claude 3 Sonnet by Anthropic"
    input_cost_per_1k: 0.003
    output_cost_per_1k: 0.015
    max_tokens: 200000
    context_window: 200000

  claude-3-haiku:
    provider: anthropic
    tokenizer: claude
    description: "Claude 3 Haiku by Anthropic"
    input_cost_per_1k: 0.00025
    output_cost_per_1k: 0.00125
    max_tokens: 200000
    context_window: 200000

  # Meta models
  llama-2-7b:
    provider: meta
    tokenizer: llama
    description: "Llama 2 7B by Meta"
    input_cost_per_1k: 0.0
    output_cost_per_1k: 0.0
    max_tokens: 4096
    context_window: 4096

  llama-2-13b:
    provider: meta
    tokenizer: llama
    description: "Llama 2 13B by Meta"
    input_cost_per_1k: 0.0
    output_cost_per_1k: 0.0
    max_tokens: 4096
    context_window: 4096

  llama-2-70b:
    provider: meta
    tokenizer: llama
    description: "Llama 2 70B by Meta"
    input_cost_per_1k: 0.0
    output_cost_per_1k: 0.0
    max_tokens: 4096
    context_window: 4096

  llama-3-8b:
    provider: meta
    tokenizer: llama
    description: "Llama 3 8B by Meta"
    input_cost_per_1k: 0.0
    output_cost_per_1k: 0.0
    max_tokens: 8192
    context_window: 8192

  llama-3-70b:
    provider: meta
    tokenizer: llama
    description: "Llama 3 70B by Meta"
    input_cost_per_1k: 0.0
    output_cost_per_1k: 0.0
    max_tokens: 8192
    context_window: 8192

  # Google models
  gemini-pro:
    provider: google
    tokenizer: gemini
    description: "Gemini Pro by Google"
    input_cost_per_1k: 0.00125
    output_cost_per_1k: 0.00375
    max_tokens: 32768
    context_window: 32768

  gemini-ultra:
    provider: google
    tokenizer: gemini
    description: "Gemini Ultra by Google"
    input_cost_per_1k: 0.00375
    output_cost_per_1k: 0.01125
    max_tokens: 32768
    context_window: 32768

  # Mistral models
  mistral-small:
    provider: mistral
    tokenizer: mistral
    description: "Mistral Small by Mistral AI"
    input_cost_per_1k: 0.002
    output_cost_per_1k: 0.006
    max_tokens: 32768
    context_window: 32768

  mistral-medium:
    provider: mistral
    tokenizer: mistral
    description: "Mistral Medium by Mistral AI"
    input_cost_per_1k: 0.0027
    output_cost_per_1k: 0.0081
    max_tokens: 32768
    context_window: 32768

  mistral-large:
    provider: mistral
    tokenizer: mistral
    description: "Mistral Large by Mistral AI"
    input_cost_per_1k: 0.008
    output_cost_per_1k: 0.024
    max_tokens: 32768
    context_window: 32768

  # Cohere models
  command-r:
    provider: cohere
    tokenizer: cohere
    description: "Command R by Cohere"
    input_cost_per_1k: 0.0005
    output_cost_per_1k: 0.0015
    max_tokens: 128000
    context_window: 128000

  command-r-plus:
    provider: cohere
    tokenizer: cohere
    description: "Command R+ by Cohere"
    input_cost_per_1k: 0.003
    output_cost_per_1k: 0.015
    max_tokens: 128000
    context_window: 128000

  # Groq models
  llama-3-70b-groq:
    provider: groq
    tokenizer: llama
    description: "Llama 3 70B optimized by Groq"
    input_cost_per_1k: 0.0
    output_cost_per_1k: 0.0
    max_tokens: 8192
    context_window: 8192

  mixtral-8x7b-groq:
    provider: groq
    tokenizer: mistral
    description: "Mixtral 8x7B optimized by Groq"
    input_cost_per_1k: 0.0
    output_cost_per_1k: 0.0
    max_tokens: 32768
    context_window: 32768

# Shortcuts for generic model types
aliases:
  gpt: gpt-3.5-turbo
  claude: claude-3-sonnet
  llama: llama-3-70b
  gemini: gemini-pro
  mistral: mistral-medium
  cohere: command-r-plus