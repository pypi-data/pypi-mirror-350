{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightlyTrain with Ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will demonstrate how you can use Lightly**Train** to pretrain a YOLO11s model from [Ultralytics](https://github.com/ultralytics/ultralytics). To this end, we will first use the raw images (**no labels**) from the [PASCAL Visual Object Classes (VOC) dataset](http://host.robots.ox.ac.uk/pascal/VOC/) and pretrain for 100 epochs. After pretraining, we will show that we can fine-tune the model to the task of object dection and significantly outperform training from scratch in terms of *acccuracy* and *convergence speed*.\n",
    "\n",
    "In order to get started, let's first make sure that:\n",
    " - We installed all required packages. \n",
    " - We have the dataset downloaded and ready in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "The required packages include `lightly-train` with support for `ultralytics`, these are the packages we need for pretraining and finetuning. \n",
    "\n",
    "As utilities, we'll also install Roboflow's `supervision`, which let's us easily visualize a few annotated data samples, as well as `ipywidgets`, which enhances a few interactive elements of this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"lightly-train[ultralytics]\" \"supervision==0.25.1\" ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important**: LightlyTrain is officially supported on\n",
    "> - Linux: CPU or CUDA\n",
    "> - MacOS: CPU only\n",
    "> - Windows (experimental): CPU or CUDA\n",
    "> \n",
    "> We are planning to support MPS for MacOS.\n",
    "> \n",
    "> Check the [installation instructions](https://docs.lightly.ai/train/stable/installation.html) for more details on installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "We can directly use Ultralytics' `check_det_dataset` function to download the VOC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.data.utils import check_det_dataset\n",
    "\n",
    "dataset = check_det_dataset(\"VOC.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultralytics always uses a fixed directory to save your datasets and you can fetch the location through their `settings` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import settings\n",
    "\n",
    "settings[\"datasets_dir\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset directory is now ready under the path from above and will have the following structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "datasets/VOC\n",
    "‚îú‚îÄ‚îÄ images\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ test2007\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train2007\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train2012\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ val2007\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ val2012\n",
    "‚îî‚îÄ‚îÄ labels\n",
    "    ‚îú‚îÄ‚îÄ test2007\n",
    "    ‚îú‚îÄ‚îÄ train2007\n",
    "    ‚îú‚îÄ‚îÄ train2012\n",
    "    ‚îú‚îÄ‚îÄ val2007\n",
    "    ‚îî‚îÄ‚îÄ val2012\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Inspection\n",
    "Before we start, let's quickly also look at a few images of the training set, together with their annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import supervision as sv\n",
    "import yaml\n",
    "\n",
    "detections = sv.DetectionDataset.from_yolo(\n",
    "    data_yaml_path=dataset[\"yaml_file\"],\n",
    "    images_directory_path=f\"{settings['datasets_dir']}/VOC/images/train2012\",\n",
    "    annotations_directory_path=f\"{settings['datasets_dir']}/VOC/labels/train2012\",\n",
    ")\n",
    "\n",
    "with open(dataset[\"yaml_file\"], \"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "names = data[\"names\"]\n",
    "\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "detections = [detections[random.randint(0, len(detections))] for _ in range(4)]\n",
    "\n",
    "for i, (path, image, annotation) in enumerate(detections):\n",
    "    annotated_image = box_annotator.annotate(scene=image, detections=annotation)\n",
    "    annotated_image = label_annotator.annotate(\n",
    "        scene=annotated_image,\n",
    "        detections=annotation,\n",
    "        labels=[names[elem] for elem in annotation.class_id],\n",
    "    )\n",
    "    ax[i].imshow(annotated_image[..., ::-1])\n",
    "    ax[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining with Lightly**Train**\n",
    "Pretraining with Lightly**Train** could not be easier, you just pass the following parameters: \n",
    " - `out`: you simply state where you want your logs and exported model to go to\n",
    " - `model`: the model that you want to train, e.g. `yolo11s` from Ultralytics\n",
    " - `data`: the path to a folder with images\n",
    "\n",
    " Your data is simply assumed to be an arbitrarily nested folder; LightlyTrain with find all images on its own and since there are no labels required there is no danger of ever using false labels! üïµÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "It is highly recommended to run this pretraining on a GPU, expect about 60min of training time on Colab's free version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightly_train\n",
    "\n",
    "lightly_train.train(\n",
    "    out=\"logs/pretrain_yolo11s\",\n",
    "    data=f\"{settings['datasets_dir']}/VOC/images/train2012\",\n",
    "    model=\"ultralytics/yolo11s.yaml\",\n",
    "    overwrite=True,  # we allow overwriting so that you can conveniently run this cell repeatedly\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning with Ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the pretrained model has been exported, we will further fine-tune the model on the task of object detection. The exported model already has exactly the format that Ultralytics' YOLO expects, so we can get started with only a few lines! ‚ö°Ô∏è In addition to fine-tuning the pretrained model we will also train a model that we initialize with random weights. This will let us compare the performance between the two, and show the great benefits of pretraining.\n",
    "\n",
    "Expect again a run-time of around 1h each for fine-tuning from the pretrained model as well as fine-tuning from randomly initialized weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model_pretrained = YOLO(\"logs/pretrain_yolo11s/exported_models/exported_last.pt\")\n",
    "model_pretrained.train(\n",
    "    data=\"VOC.yaml\",\n",
    "    epochs=30,\n",
    "    project=\"logs/voc_yolo11s\",\n",
    "    name=\"from_pretrained\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to fine-tuning the pretrained model we will also train a model that we initialize with random weights. This will let us compare the performance between the two, and show the great benefits of pretraining. ‚ö°Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scratch = YOLO(\"yolo11s.yaml\")\n",
    "model_scratch.train(\n",
    "    data=\"VOC.yaml\",\n",
    "    epochs=30,\n",
    "    project=\"logs/voc_yolo11s\",\n",
    "    name=\"from_scratch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "Congratulations, you made it almost to the end! üéâ The last thing we'll do is to analyze the performance between the two. A very common metric to measure the performance of object detectors is the `mAP50-95` which we plot in the next cell, for both the pretrained model and the model that we trained from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "res_scratch = pd.read_csv(\"logs/voc_yolo11s/from_scratch/results.csv\")\n",
    "res_finetune = pd.read_csv(\"logs/voc_yolo11s/from_pretrained/results.csv\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(res_scratch[\"epoch\"], res_scratch[\"metrics/mAP50-95(B)\"], label=\"scratch\")\n",
    "ax.plot(res_finetune[\"epoch\"], res_finetune[\"metrics/mAP50-95(B)\"], label=\"finetune\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"mAP50-95\")\n",
    "max_pretrained = res_finetune[\"metrics/mAP50-95(B)\"].max()\n",
    "max_scratch = res_scratch[\"metrics/mAP50-95(B)\"].max()\n",
    "ax.set_title(\n",
    "    f\"Pretraining is {(max_pretrained - max_scratch) / max_scratch * 100:.2f}% better than scratch\"\n",
    ")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the pretrained model outperforms the model trained from scratch by a significant margin, thanks to the magic of Lightly**Train**. Now you are ready to leverage it to pretrain on your own data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
